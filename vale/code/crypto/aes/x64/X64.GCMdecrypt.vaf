include "../../../arch/x64/X64.Vale.InsBasic.vaf"
include "../../../arch/x64/X64.Vale.InsMem.vaf"
include "../../../arch/x64/X64.Vale.InsStack.vaf"
include "../../../arch/x64/X64.Vale.InsVector.vaf"
include "../../../lib/util/x64/X64.Stack.vaf"
include "X64.AES.vaf"
include "X64.GF128_Mul.vaf"
include "X64.GCTR.vaf"
include "X64.GHash.vaf"
include "X64.GCMencrypt.vaf"
include{:fstar}{:open} "Opaque_s"
include{:/*TODO*/fstar}{:open} "FStar.Seq.Base"
include{:fstar}{:open} "Words_s"
include{:fstar}{:open} "Words.Seq_s"
include{:fstar}{:open} "Types_s"
include{:fstar}{:open} "Arch.Types"
include{:fstar}{:open} "AES_s"
include{:fstar}{:open} "GCTR_s"
include{:fstar}{:open} "GCTR"
include{:fstar}{:open} "GCM"
include{:fstar}{:open} "GHash_s"
include{:fstar}{:open} "GHash"
include{:fstar}{:open} "GCM_s"
include{:fstar}{:open} "GF128_s"
include{:fstar}{:open} "GF128"
include{:fstar}{:open} "X64.Poly1305.Math"
include{:fstar}{:open} "GCM_helpers"
include{:fstar}{:open} "Workarounds"
include{:fstar}{:open} "X64.Machine_s"
include{:fstar}{:open} "X64.Memory"
include{:fstar}{:open} "X64.Stack_i"
include{:fstar}{:open} "X64.Vale.State"
include{:fstar}{:open} "X64.Vale.Decls"
include{:fstar}{:open} "X64.Vale.QuickCode"
include{:fstar}{:open} "X64.Vale.QuickCodes"
include{:fstar}{:open} "X64.CPU_Features_s"

module X64.GCMdecrypt

#verbatim{:interface}{:implementation}
module GHash = GHash
module GCTR = GCTR
open Opaque_s
open FStar.Seq
open Words_s
open Words.Seq_s
open Types_s
open Arch.Types
open AES_s
open GCTR_s
open GCTR
open GCM
open GHash_s
open GHash
open GCM_s
open X64.AES
open GF128_s
open GF128
open X64.Poly1305.Math
open GCM_helpers
open Workarounds
open X64.GHash
open X64.GCTR
open X64.Machine_s
open X64.Memory
open X64.Stack_i
open X64.Vale.State
open X64.Vale.Decls
open X64.Vale.InsBasic
open X64.Vale.InsMem
open X64.Vale.InsStack
open X64.Vale.InsVector
open X64.Vale.InsAes
open X64.Vale.QuickCode
open X64.Vale.QuickCodes
open X64.GF128_Mul
open X64.GCMencrypt
open X64.Stack
open X64.CPU_Features_s
#endverbatim

///////////////////////////
// GCM
///////////////////////////

#reset-options "--z3rlimit 30"
procedure {:quick} gcm_one_pass_blocks(
    inline alg:algorithm,
    ghost in_b:buffer128,
    ghost out_b:buffer128,
    ghost key:seq(nat32),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    lets in_ptr @= rax; out_ptr @= rbx; len @= rcx; icb @= xmm7; mask @= xmm8; hash @= xmm1; one @= xmm10; h @= xmm11;

    reads
        r8; in_ptr; out_ptr; len; mask; h; memTaint;

    modifies
        rdx; r9; r10; r12; xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; xmm6; icb; one; mem; efl;

    requires
        // GCTR reqs
        buffers_disjoint128(in_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        validSrcAddrs128(mem, in_ptr, in_b, len, memTaint, Secret);
        validDstAddrs128(mem, out_ptr, out_b, len, memTaint, Secret);
        in_ptr  + 16 * len < pow2_64;
        out_ptr + 16 * len < pow2_64;
        buffer_length(in_b) == buffer_length(out_b) /\ 256 * buffer_length(in_b) < pow2_32;
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

        // AES reqs
        aesni_enabled;
        alg = AES_128 || alg = AES_256;
        is_aes_key_LE(alg, key);
        length(round_keys) == nr(alg) + 1;
        round_keys == key_to_round_keys_LE(alg, key);
        validSrcAddrs128(mem, r8, keys_b, nr(alg) + 1, memTaint, Secret);
        buffer128_as_seq(mem, keys_b) == round_keys;

        pclmulqdq_enabled;
    ensures
        modifies_buffer128(out_b, old(mem), mem);
        validSrcAddrs128(mem, out_ptr, out_b, len, memTaint, Secret);

        r9  ==  in_ptr + 16 * len;
        r10 == out_ptr + 16 * len;

        // GCTR
        gctr_partial(alg, len, buffer128_as_seq(mem, in_b), buffer128_as_seq(mem, out_b), key, old(icb));
        icb == inc32(old(icb), old(len));

        // GHash
        len == 0 ==> hash == old(hash);
        len > 0 ==> length(buffer128_as_seq(mem, out_b)) > 0 /\ hash == ghash_incremental(reverse_bytes_quad32(h), old(hash), slice_work_around(old(buffer128_as_seq(mem, in_b)), len));
{
    Mov64(rdx, 0);
    Mov64(r9, in_ptr);
    Mov64(r10, out_ptr);

    // Initialize counter
    ZeroXmm(one);
    PinsrdImm(one, 1, 0, r12);

    while (rdx != len)
        invariant
            //////////////////// Basic indexing //////////////////////
            0 <= rdx <= len;
            r9 == in_ptr + 16 * rdx;
            r10 == out_ptr + 16 * rdx;
            icb == inc32(old(icb), rdx);

            //////////////////// From requires //////////////////////
            // GCTR reqs
            buffers_disjoint128(in_b, out_b);
            buffers_disjoint128(keys_b, out_b);
            validSrcAddrs128(mem, in_ptr, in_b, len, memTaint, Secret);
            validDstAddrs128(mem, out_ptr, out_b, len, memTaint, Secret);
            in_ptr  + 16 * len < pow2_64;
            out_ptr + 16 * len < pow2_64;
            buffer_length(in_b) == buffer_length(out_b);

            // AES reqs
            aesni_enabled;
            alg = AES_128 || alg = AES_256;
            is_aes_key_LE(alg, key);
            length(round_keys) == nr(alg) + 1;
            round_keys == key_to_round_keys_LE(alg, key);
            validSrcAddrs128(mem, r8, keys_b, nr(alg) + 1, memTaint, Secret);
            buffer128_as_seq(mem, keys_b) == round_keys;

            //////////////////// GCTR invariants //////////////////////
            mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
            one == Mkfour(1, 0, 0, 0);

            //////////////////// Postcondition goals //////////////////////
            modifies_buffer128(out_b, old(mem), mem);
            validSrcAddrs128(mem, out_ptr, out_b, len, memTaint, Secret);
            gctr_partial(alg, rdx, buffer128_as_seq(mem, in_b), buffer128_as_seq(mem, out_b), key, old(icb));

            pclmulqdq_enabled;
            rdx == 0 ==> hash == old(hash);
            rdx > 0 ==> hash == ghash_incremental(reverse_bytes_quad32(h), old(hash), slice_work_around(buffer128_as_seq(mem, in_b), rdx));

//            forall j :: 0 <= j < rdx ==>
//                        buffer128_read(out_b, j, mem) ==
//                        quad32_xor(index_workaround(buffer128_as_seq(mem, in_b), j), aes_encrypt_LE(AES_128, key, inc32(old(icb), j)));

        decreases
            len - rdx;
    {
        Load128_buffer(xmm0, r9, 0, Secret, in_b, rdx);

        // Update our hash
        Mov128(xmm2, xmm0); // Hash expects input in xmm2
        compute_ghash_incremental_register(); // clobbers xmm1-6

        // Decrypt
        Mov128(xmm3, xmm0); // Save a copy, since AES expects its input in xmm0

        Mov128(xmm0, icb);
        Pshufb(xmm0, mask);
        AESEncryptBlock(alg, reverse_bytes_quad32(icb), key, round_keys, keys_b); // Clobbers xmm0 and xmm2
        reveal aes_encrypt_LE_def;

        Pxor(xmm3, xmm0);
        Store128_buffer(r10, xmm3, 0, Secret, out_b, rdx);


        Add64(rdx, 1);
        Add64(r9, 16);
        Add64(r10, 16);
        Inc32(icb, one);
        reveal ghash_incremental_def;
    }
}


#reset-options "--z3rlimit 30"
procedure {:quick} gcm_one_pass(
    inline alg:algorithm,
    ghost in_b:buffer128,
    ghost out_b:buffer128,
    ghost key:seq(nat32),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    lets in_ptr @= rax; out_ptr @= rbx; num_bytes @= rcx; icb @= xmm7; mask @= xmm8; hash @= xmm1; one @= xmm10; h @= xmm11;

    reads
        r8; out_ptr; mask; h; memTaint;

    modifies
        in_ptr; num_bytes; rdx; rsi; r9; r10; r12; xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; xmm6; icb; one; mem; efl;

    requires
        // GCTR reqs
        buffers_disjoint128(in_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        validSrcAddrs128(mem, in_ptr,  in_b,  bytes_to_quad_size(num_bytes), memTaint, Secret);
        validDstAddrs128(mem, out_ptr, out_b, bytes_to_quad_size(num_bytes), memTaint, Secret);
        in_ptr  + 16 * bytes_to_quad_size(num_bytes) < pow2_64;
        out_ptr + 16 * bytes_to_quad_size(num_bytes) < pow2_64;
        buffer_length(in_b)  == buffer_length(out_b);
        buffer_length(out_b) == bytes_to_quad_size(num_bytes);
        256 * buffer_length(in_b) < pow2_32;
        4096 * num_bytes < pow2_32;
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);

        // AES reqs
        aesni_enabled;
        alg = AES_128 || alg = AES_256;
        is_aes_key_LE(alg, key);
        length(round_keys) == nr(alg) + 1;
        round_keys == key_to_round_keys_LE(alg, key);
        validSrcAddrs128(mem, r8, keys_b, nr(alg) + 1, memTaint, Secret);
        buffer128_as_seq(mem, keys_b) == round_keys;

        pclmulqdq_enabled;
    ensures
        modifies_buffer128(out_b, old(mem), mem);
        validSrcAddrs128(mem, out_ptr, out_b, bytes_to_quad_size(old(num_bytes)), memTaint, Secret);

        // GCTR
        let cipher := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem),  in_b)), old(num_bytes));
        let plain  := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem,      out_b)), old(num_bytes));
        plain == gctr_encrypt_LE(old(icb), make_gctr_plain_LE(cipher), alg, key);

        //icb == inc32(old(icb), bytes_to_quad_size(old(num_bytes)));
        icb.lo1 == old(icb.lo1);
        icb.hi2 == old(icb.hi2);
        icb.hi3 == old(icb.hi3);

        // GHash
        old(num_bytes) == 0 ==> hash == old(hash);

        let cipher_padded_bytes := pad_to_128_bits(cipher);
        let cipher_padded_quads := le_bytes_to_seq_quad32(cipher_padded_bytes);
        old(num_bytes) > 0 ==> length(cipher_padded_quads) > 0 /\
            hash == ghash_incremental(reverse_bytes_quad32(h), old(hash), cipher_padded_quads);
{
    if (num_bytes > 0) {
        lemma_poly_bits64();
        Mov64(rsi, num_bytes);
        And64(rsi, 15);
        assert rsi == num_bytes % 16;
        Shr64(num_bytes, 4);
        ghost var num_blocks := old(num_bytes) / 16;
        assert rcx == num_blocks;

        gcm_one_pass_blocks(alg, in_b, out_b, key, round_keys, keys_b);
        assert icb == inc32(old(icb), num_blocks);

        if (rsi == 0) {
            gctr_bytes_no_extra(alg, old(icb), in_b, out_b, key, round_keys, keys_b, old(in_ptr), old(out_ptr), old(num_bytes));
            ghash_incremental_bytes_no_extra(in_b, old(hash), old(in_ptr), old(num_bytes), hash, reverse_bytes_quad32(h));
            bytes_to_quad_size_no_extra_bytes(old(num_bytes));
            //assert icb == inc32(old(icb), bytes_to_quad_size(old(num_bytes)));

            assert buffer128_as_seq(old(mem), in_b) == buffer128_as_seq(mem, in_b);       // OBSERVE
        } else {
            // TODO: Should update gcm_one_pass_blocks and gctr_* to give their results in terms of old(mem) and in_b, instead of mem
//            assert buffer128_as_seq(old(mem), in_b) == buffer128_as_seq(mem, in_b);       // OBSERVE
            Mov128(xmm3, hash); // Save hash b/c gctr_bytes_extra will clobber it
            gctr_bytes_extra(alg, old(icb), in_b, out_b, key, round_keys, keys_b, old(in_ptr), old(out_ptr), old(num_bytes));
            Mov64(rax, rsi);    // ghash_incremental_bytes_extra expects num_bytes % 16 in rax
            //Mov64(r9, r10);     // ghash_incremental_bytes_extra expects old(out_ptr) + 16 * num_blocks in r9
            Mov128(hash, xmm3);
            // TODO: Can optimize here by skipping the memory load in ghash_incremental_bytes_extra
            //       and reading from xmm1 directly.  It's probably still in cache though
            ghash_incremental_bytes_extra(in_b, old(in_ptr), old(hash), old(num_bytes));
//            assert buffer128_as_seq(old(mem), in_b) == buffer128_as_seq(mem, in_b);       // OBSERVE
        }

        let cipher := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), in_b)), old(num_bytes));
        let cipher_padded_bytes := pad_to_128_bits(cipher);
        let cipher_padded_quads := le_bytes_to_seq_quad32(cipher_padded_bytes);
        assert length(cipher_padded_quads) > 0 by {reveal_le_bytes_to_seq_quad32();}
    }

    // Handle the else case:
    ghost var cipher := buffer128_as_seq(old(mem),  in_b);
    ghost var plain  := buffer128_as_seq(mem,      out_b);
    gctr_encrypt_empty(old(icb), cipher, plain, alg, key);

}

#reset-options "--z3rlimit 30"
procedure {:quick} gcm_core_part1(
    inline alg:algorithm,
    ghost iv_BE:quad32,
    ghost cipher_b:buffer128,
    ghost auth_b:buffer128,
    ghost out_b:buffer128,
    ghost key:seq(nat32),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    ) returns (
    ghost h:quad32,
    ghost y_0:quad32,
    ghost y_auth:quad32,
    ghost y_cipher:quad32
    )

    lets cipher_ptr @= r14; out_ptr @= rbx; keys_ptr @= r8; auth_ptr @= rax;
    cipher_num_bytes @= r13; auth_num_bytes @= r11; mask @= xmm8;
    iv @= xmm7;

    reads
        out_ptr; cipher_ptr; keys_ptr; cipher_num_bytes; auth_num_bytes;  memTaint;

    modifies
        rax; rcx; rdx; rsi; r9; r10; r12; xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; xmm10; xmm11; iv; mask;
        xmm6;
        mem; efl;


    requires
        // GCM reqs
        iv == reverse_bytes_quad32(iv_BE);
        buffers_disjoint128(cipher_b, out_b);
        buffers_disjoint128(auth_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        validSrcAddrs128(mem, cipher_ptr, cipher_b, bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);
        validSrcAddrs128(mem, auth_ptr,  auth_b,  bytes_to_quad_size(auth_num_bytes), memTaint, Secret);
        validDstAddrs128(mem, out_ptr,   out_b,   bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);
        cipher_ptr + 16 * bytes_to_quad_size(cipher_num_bytes) < pow2_64;
        auth_ptr  + 16 * bytes_to_quad_size(auth_num_bytes)  < pow2_64;
        out_ptr   + 16 * bytes_to_quad_size(cipher_num_bytes) < pow2_64;
        buffer_length(cipher_b) == buffer_length(out_b);
        buffer_length(out_b) == bytes_to_quad_size(cipher_num_bytes);
        buffer_length(auth_b) == bytes_to_quad_size(auth_num_bytes);
        256 * buffer_length(cipher_b) < pow2_32;
        4096 * cipher_num_bytes < pow2_32;
        4096 * auth_num_bytes < pow2_32;

        // To simplify length calculations, restrict auth and cipher length further
        256 * bytes_to_quad_size(auth_num_bytes)  < pow2_32;
        256 * bytes_to_quad_size(cipher_num_bytes) < pow2_32;

        // AES reqs
        aesni_enabled;
        alg = AES_128 || alg = AES_256;
        is_aes_key_LE(alg, key);
        length(round_keys) == nr(alg) + 1;
        round_keys == key_to_round_keys_LE(alg, key);
        validSrcAddrs128(mem, keys_ptr, keys_b, nr(alg) + 1, memTaint, Secret);
        buffer128_as_seq(mem, keys_b) == round_keys;

        pclmulqdq_enabled;
    ensures
        modifies_buffer128(out_b, old(mem), mem);
        validSrcAddrs128(mem, out_ptr, out_b, bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);

        // Main result
        let auth   := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)),  old(auth_num_bytes));
        let cipher  := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), cipher_b)), old(cipher_num_bytes));
        let plain := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem,      out_b)),   old(cipher_num_bytes));

//        // TODO: First two clauses work around Vale's type limitations
        4096 * length(cipher) < pow2_32 /\
        4096 * length(auth) < pow2_32 /\
        plain == gcm_encrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), cipher, auth)._1;

        // Intermediate hash state needed for the next step
        let auth_padded_bytes := pad_to_128_bits(auth);
        let auth_padded_quads := le_bytes_to_seq_quad32(auth_padded_bytes);

        let cipher_padded_bytes := pad_to_128_bits(cipher);
        let cipher_padded_quads := le_bytes_to_seq_quad32(cipher_padded_bytes);

        y_0 == Mkfour(0, 0, 0, 0);
        y_auth == ghash_incremental0(reverse_bytes_quad32(h), y_0, auth_padded_quads);
        y_cipher == ghash_incremental0(reverse_bytes_quad32(h), y_auth, cipher_padded_quads);
        xmm1 == y_cipher;

        // Intermediate IV state
        iv == Mkfour(iv.lo0, iv_BE.lo1, iv_BE.hi2, iv_BE.hi3);

        // Other intermediate facts
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
        xmm11 == h;
        h == reverse_bytes_quad32(aes_encrypt_LE(alg, key, y_0));
{
    InitPshufbMask(xmm8, r12);
    // let h = aes_encrypt_LE alg key (Mkfour 0 0 0 0) in
    ZeroXmm(xmm0);
    AESEncryptBlock(alg, xmm0, key, round_keys, keys_b); // h = xmm0 = aes_encrypt_LE alg key (Mkfour 0 0 0 0) in
    Pshufb(xmm0, xmm8);
    h := xmm0;
    Mov128(xmm11, xmm0);     // Save a copy of h

    // let j0_BE = Mkfour 1 iv_BE.lo1 iv_BE.hi2 iv_BE.hi3 in
    // (inc32 j0 1)
    Pshufb(iv, xmm8); // to big endian
    PinsrdImm(iv, 2, 0, r12);
    // assert iv == inc32(Mkfour(1, iv_BE.lo1, iv_BE.hi2, iv_BE.hi3), 1);

    // Compute the hashes incrementally, starting with auth data
    //Mov64(rax, auth_ptr);
    //Mov64(rcx, auth_num_bytes);
    ZeroXmm(xmm1);
    y_0 := Mkfour(0, 0, 0, 0);
    // assert xmm1 == y_0;
    ghash_incremental_bytes(auth_b);
    y_auth := xmm1;

//    ghost var auth_bytes := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem, auth_b)), auth_num_bytes);
//    ghost var auth_padded_bytes := pad_to_128_bits(auth_bytes);
//    ghost var auth_padded_quads := le_bytes_to_seq_quad32(auth_padded_bytes);
    ghost var cipher := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), cipher_b)), old(cipher_num_bytes));
    ghost var auth   := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)),   old(auth_num_bytes));
//
//    assert y_0 == Mkfour(0,0,0,0);
//    assert y_auth   == ghash_incremental0(reverse_bytes_quad32(h), y_0, auth_padded_quads);
//
    // let c = gctr_encrypt_LE (inc32 j0 1) p alg key in
    Mov64(rax, cipher_ptr);
    //Mov64(rbx, out_ptr);
    Mov64(rcx, cipher_num_bytes);
    ghost var icb_enc := iv;

    assert icb_enc == inc32 (Mkfour(1,iv_BE.lo1,iv_BE.hi2,iv_BE.hi3), 1);     // Passes -- REVIEW: Seems to be necessary for it to pass at the end of the procedure; unclear why that could possibly be the case
    gcm_one_pass(alg, cipher_b, out_b, key, round_keys, keys_b);
    y_cipher := xmm1;
    ghost var plain := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem, out_b)), old(cipher_num_bytes));
    gcm_encrypt_LE_fst_helper(icb_enc, iv_BE, cipher, auth, plain, alg, key);
    reveal_le_bytes_to_seq_quad32();
}


#reset-options "--z3rlimit 10"
procedure {:quick} gcm_make_length_quad()
    lets cipher_num_bytes @= r13; auth_num_bytes @= r11; mask @= xmm8;
    reads cipher_num_bytes; auth_num_bytes; mask;
    modifies xmm2; rax; efl;
    requires
        mask == Mkfour(0x0C0D0E0F, 0x08090A0B, 0x04050607, 0x00010203);
        8 * cipher_num_bytes < pow2_32;
        8 * auth_num_bytes < pow2_32;
    ensures
        8 * old(cipher_num_bytes) < pow2_32;
        8 * old(auth_num_bytes) < pow2_32;
        xmm2 == reverse_bytes_quad32(Mkfour(#nat32(8 * old(cipher_num_bytes)), 0, #nat32(8 * old(auth_num_bytes)), 0));
{
    // Prepare length fields
    ZeroXmm(xmm2);
    Mov64(rax, cipher_num_bytes);
    IMul64(rax, 8);
    Pinsrd(xmm2, rax, 0);
    Mov64(rax, auth_num_bytes);
    IMul64(rax, 8);
    Pinsrd(xmm2, rax, 2);
    // assert xmm2 == Mkfour(#nat32(8 * cipher_num_bytes), 0, #nat32(8 * auth_num_bytes), 0);     // Passes when ghost vars above are removed

    Pshufb(xmm2, xmm8);
}


#reset-options "--z3rlimit 30"
procedure {:quick} gcm_core(
    inline alg:algorithm,
    ghost iv_BE:quad32,
    ghost cipher_b:buffer128,
    ghost auth_b:buffer128,
    ghost out_b:buffer128,
    ghost key:seq(nat32),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )

    lets cipher_ptr @= r14; out_ptr @= rbx; keys_ptr @= r8; auth_ptr @= rax;
    cipher_num_bytes @= r13; auth_num_bytes @= r11; mask @= xmm8;
    iv @= xmm7;

    reads
        out_ptr; cipher_ptr; keys_ptr; cipher_num_bytes; auth_num_bytes; memTaint;

    modifies
        rax; rcx; rdx; rsi; r9; r10; r12; xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; xmm10; xmm11; iv; mask;
        xmm6;
        mem; efl;


    requires
        // GCM reqs
        iv == reverse_bytes_quad32(iv_BE);
        buffers_disjoint128(cipher_b, out_b);
        buffers_disjoint128(auth_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        validSrcAddrs128(mem, cipher_ptr, cipher_b, bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);
        validSrcAddrs128(mem, auth_ptr,  auth_b,  bytes_to_quad_size(auth_num_bytes), memTaint, Secret);
        validDstAddrs128(mem, out_ptr,   out_b,   bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);
        cipher_ptr + 16 * bytes_to_quad_size(cipher_num_bytes) < pow2_64;
        auth_ptr  + 16 * bytes_to_quad_size(auth_num_bytes)  < pow2_64;
        out_ptr   + 16 * bytes_to_quad_size(cipher_num_bytes) < pow2_64;
        buffer_length(cipher_b) == buffer_length(out_b);
        buffer_length(out_b) == bytes_to_quad_size(cipher_num_bytes);
        buffer_length(auth_b) == bytes_to_quad_size(auth_num_bytes);
        256 * buffer_length(cipher_b) < pow2_32;
        4096 * cipher_num_bytes < pow2_32;
        4096 * auth_num_bytes < pow2_32;

        // To simplify length calculations, restrict auth and cipher length further
        256 * bytes_to_quad_size(auth_num_bytes)  < pow2_32;
        256 * bytes_to_quad_size(cipher_num_bytes) < pow2_32;

        // AES reqs
        aesni_enabled;
        alg = AES_128 || alg = AES_256;
        is_aes_key_LE(alg, key);
        length(round_keys) == nr(alg) + 1;
        round_keys == key_to_round_keys_LE(alg, key);
        validSrcAddrs128(mem, keys_ptr, keys_b, nr(alg) + 1, memTaint, Secret);
        buffer128_as_seq(mem, keys_b) == round_keys;

        pclmulqdq_enabled;
    ensures
        modifies_buffer128(out_b, old(mem), mem);
        validSrcAddrs128(mem, out_ptr, out_b, bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);

        let auth   := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)),  old(auth_num_bytes));
        let cipher  := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), cipher_b)), old(cipher_num_bytes));
        let plain := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem,      out_b)),   old(cipher_num_bytes));

//        // TODO: First two clauses work around Vale's type limitations
        4096 * length(cipher) < pow2_32 /\
        4096 * length(auth) < pow2_32 /\
        plain == gcm_encrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), cipher, auth)._1 /\
        //le_quad32_to_bytes(xmm1) == gcm_encrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), cipher, auth)._2;
        le_quad32_to_bytes(xmm1) == gcm_decrypt_LE_tag(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), cipher, auth);
{
    (ghost var h), (ghost var y_0), (ghost var y_auth), (ghost var y_cipher) := gcm_core_part1(alg, iv_BE, cipher_b, auth_b, out_b, key, round_keys, keys_b);

    gcm_make_length_quad();
    ghost var length_quad32 := xmm2;

    compute_ghash_incremental_register();
    ghost var y_final := xmm1;

    // Invoke lemma showing that incremental hashing works
    ghost var auth_bytes := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)), old(auth_num_bytes));
    ghost var auth_padded_quads := le_bytes_to_seq_quad32(pad_to_128_bits(auth_bytes));

    ghost var cipher := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), cipher_b)), old(cipher_num_bytes));
    ghost var cipher_padded_quads := le_bytes_to_seq_quad32(pad_to_128_bits(cipher));
    lemma_hash_append3(reverse_bytes_quad32(h), y_0, y_auth, y_cipher, y_final,
                       auth_padded_quads, //buffer128_as_seq(mem, auth_b),
                       cipher_padded_quads, //buffer128_as_seq(mem, out_b),
                       create(1, length_quad32));

    PinsrdImm(iv, 1, 0, r12);   // Reconstruct j0 (this is all we need, since gctr_core says it only changes iv.lo0)

    // Encrypt the hash value with gctr_register; result goes in xmm1
    gctr_register(alg, key, round_keys, keys_b); // Encrypt using j0 and xmm0 = hash_value

    ghost var plain := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem, out_b)), old(cipher_num_bytes));

    le_seq_quad32_to_bytes_of_singleton(xmm1);

    //gcm_encrypt_LE_snd_helper(iv_BE, length_quad32, y_final, xmm1, cipher, auth_bytes, plain, alg, key);
}

procedure {:quick} gcm_decrypt_stdcall_inner_inner(
    inline alg:algorithm,
    ghost args_b:buffer64,
    ghost iv_BE:quad32,

    ghost cipher_b:buffer128,
    ghost auth_b:buffer128,
    ghost iv_b:buffer128,
    ghost out_b:buffer128,
    ghost tag_b:buffer128,

    ghost key:seq(nat32),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    reads memTaint;
    modifies
        rax; rbx; rcx; rdx; rsi; r8; r9; r10; r11; r12; r13; r14; r15; xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; xmm6; xmm7; xmm8; xmm10; xmm11;
        mem; efl;
    lets
        args_ptr := r9;

        cipher_ptr         := buffer64_read(args_b, 0, mem);
        cipher_num_bytes   := buffer64_read(args_b, 1, mem);
        auth_ptr          := buffer64_read(args_b, 2, mem);
        auth_num_bytes    := buffer64_read(args_b, 3, mem);
        iv_ptr            := buffer64_read(args_b, 4, mem);
        expanded_key_ptr  := buffer64_read(args_b, 5, mem);
        out_ptr           := buffer64_read(args_b, 6, mem);
        tag_ptr           := buffer64_read(args_b, 7, mem);

    requires
        validSrcAddrs64(mem, args_ptr, args_b, 8, memTaint, Public);

        buffers_disjoint128(cipher_b, out_b);
        buffers_disjoint128(auth_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        buffers_disjoint128(out_b, tag_b);

        validSrcAddrs128(mem, cipher_ptr, cipher_b, bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);
        validSrcAddrs128(mem, auth_ptr, auth_b, bytes_to_quad_size(auth_num_bytes), memTaint, Secret);
        validSrcAddrs128(mem, iv_ptr, iv_b, 1, memTaint, Secret);
        validDstAddrs128(mem, out_ptr, out_b, bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);
        validDstAddrs128(mem, tag_ptr, tag_b, 1, memTaint, Secret);

        cipher_ptr + 16 * bytes_to_quad_size(cipher_num_bytes) < pow2_64;
        auth_ptr  + 16 * bytes_to_quad_size(auth_num_bytes)  < pow2_64;
        out_ptr   + 16 * bytes_to_quad_size(cipher_num_bytes) < pow2_64;
        buffer_length(cipher_b) == buffer_length(out_b);
        buffer_length(out_b) == bytes_to_quad_size(cipher_num_bytes);
        buffer_length(auth_b) == bytes_to_quad_size(auth_num_bytes);
        256 * buffer_length(cipher_b) < pow2_32;
        4096 * cipher_num_bytes < pow2_32;
        4096 * auth_num_bytes < pow2_32;

        iv_BE == reverse_bytes_quad32(buffer128_read(iv_b, 0, mem));

        // To simplify length calculations, restrict auth and cipher length further
        256 * bytes_to_quad_size(auth_num_bytes)  < pow2_32;
        256 * bytes_to_quad_size(cipher_num_bytes) < pow2_32;

        // AES reqs
        aesni_enabled;
        alg = AES_128 || alg = AES_256;
        is_aes_key_LE(alg, key);
        length(round_keys) == nr(alg) + 1;
        round_keys == key_to_round_keys_LE(alg, key);
        validSrcAddrs128(mem, expanded_key_ptr, keys_b, nr(alg) + 1, memTaint, Secret);
        buffer128_as_seq(mem, keys_b) == round_keys;

        pclmulqdq_enabled;
    ensures
        modifies_buffer128(out_b, old(mem), mem);

        validSrcAddrs128(mem, old(out_ptr), out_b, bytes_to_quad_size(old(cipher_num_bytes)), memTaint, Secret);

        let auth   := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)),   old(auth_num_bytes));
        let cipher := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), cipher_b)), old(cipher_num_bytes));
        let plain  := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem,      out_b)),    old(cipher_num_bytes));
        let alleged_tag := le_quad32_to_bytes(buffer128_read(tag_b, 0, old(mem)));

//        // TODO: First two clauses work around Vale's type limitations
        4096 * length(cipher) < pow2_32 /\
        4096 * length(auth) < pow2_32 /\
        plain == gcm_decrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), cipher, auth, alleged_tag)._1 /\
        //le_quad32_to_bytes(xmm1) == gcm_encrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), cipher, auth)._2;
        le_quad32_to_bytes(xmm1) == gcm_decrypt_LE_tag(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), cipher, auth);
        r15 == old(buffer64_read(args_b, 7, mem));
{
    Load64_buffer(r14, r9,  0, Public, args_b, 0);
    Load64_buffer(r13, r9,  8, Public, args_b, 1);
    Load64_buffer(rax, r9, 16, Public, args_b, 2);
    Load64_buffer(r11, r9, 24, Public, args_b, 3);
    Load64_buffer(r10, r9, 32, Public, args_b, 4);
    Load64_buffer(r8,  r9, 40, Public, args_b, 5);
    Load64_buffer(rbx, r9, 48, Public, args_b, 6);
    Load64_buffer(r15, r9, 56, Public, args_b, 7);

    // Load the IV into its XMM register
    Load128_buffer(xmm7, r10, 0, Secret, iv_b, 0);

    gcm_core(alg, iv_BE, cipher_b, auth_b, out_b, key, round_keys, keys_b);

    ghost var auth   := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)),   old(auth_num_bytes));
    ghost var cipher := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), cipher_b)), old(cipher_num_bytes));
    ghost var plain  := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem,      out_b)),    old(cipher_num_bytes));
    ghost var alleged_tag := le_quad32_to_bytes(buffer128_read(tag_b, 0, old(mem)));
    lemma_gcm_encrypt_decrypt_equiv(alg, key, iv_BE, plain, cipher, auth, alleged_tag);
}



#reset-options "--z3rlimit 20"
procedure {:quick} gcm_decrypt_stdcall_inner(
    inline alg:algorithm,
    ghost args_b:buffer64,
    ghost iv_BE:quad32,

    ghost cipher_b:buffer128,
    ghost auth_b:buffer128,
    ghost iv_b:buffer128,
    ghost out_b:buffer128,
    ghost tag_b:buffer128,

    ghost key:seq(nat32),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    reads memTaint;
    modifies
        rax; rbx; rcx; rdx; rsi; r8; r9; r10; r11; r12; r13; r14; r15; xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; xmm6; xmm7; xmm8; xmm10; xmm11;
        mem; efl;
    lets
        args_ptr := r9;

        cipher_ptr         := buffer64_read(args_b, 0, mem);
        cipher_num_bytes   := buffer64_read(args_b, 1, mem);
        auth_ptr          := buffer64_read(args_b, 2, mem);
        auth_num_bytes    := buffer64_read(args_b, 3, mem);
        iv_ptr            := buffer64_read(args_b, 4, mem);
        expanded_key_ptr  := buffer64_read(args_b, 5, mem);
        out_ptr           := buffer64_read(args_b, 6, mem);
        tag_ptr           := buffer64_read(args_b, 7, mem);

    requires
        validSrcAddrs64(mem, args_ptr, args_b, 8, memTaint, Public);

        buffers_disjoint128(cipher_b, out_b);
        buffers_disjoint128(auth_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        buffers_disjoint128(out_b, tag_b);

        validSrcAddrs128(mem, cipher_ptr, cipher_b, bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);
        validSrcAddrs128(mem, auth_ptr, auth_b, bytes_to_quad_size(auth_num_bytes), memTaint, Secret);
        validSrcAddrs128(mem, iv_ptr, iv_b, 1, memTaint, Secret);
        validDstAddrs128(mem, out_ptr, out_b, bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);
        validDstAddrs128(mem, tag_ptr, tag_b, 1, memTaint, Secret);

        cipher_ptr + 16 * bytes_to_quad_size(cipher_num_bytes) < pow2_64;
        auth_ptr  + 16 * bytes_to_quad_size(auth_num_bytes)  < pow2_64;
        out_ptr   + 16 * bytes_to_quad_size(cipher_num_bytes) < pow2_64;
        buffer_length(cipher_b) == buffer_length(out_b);
        buffer_length(out_b) == bytes_to_quad_size(cipher_num_bytes);
        buffer_length(auth_b) == bytes_to_quad_size(auth_num_bytes);
        256 * buffer_length(cipher_b) < pow2_32;
        4096 * cipher_num_bytes < pow2_32;
        4096 * auth_num_bytes < pow2_32;

        iv_BE == reverse_bytes_quad32(buffer128_read(iv_b, 0, mem));

        // To simplify length calculations, restrict auth and cipher length further
        256 * bytes_to_quad_size(auth_num_bytes)  < pow2_32;
        256 * bytes_to_quad_size(cipher_num_bytes) < pow2_32;

        // AES reqs
        aesni_enabled;
        alg = AES_128 || alg = AES_256;
        is_aes_key_LE(alg, key);
        length(round_keys) == nr(alg) + 1;
        round_keys == key_to_round_keys_LE(alg, key);
        validSrcAddrs128(mem, expanded_key_ptr, keys_b, nr(alg) + 1, memTaint, Secret);
        buffer128_as_seq(mem, keys_b) == round_keys;

        pclmulqdq_enabled;

    ensures
        modifies_buffer128(out_b, old(mem), mem);
        validSrcAddrs128(mem, old(out_ptr), out_b, bytes_to_quad_size(old(cipher_num_bytes)), memTaint, Secret);

        let auth   := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)),  old(auth_num_bytes));
        let cipher  := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), cipher_b)), old(cipher_num_bytes));
        let plain := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem,      out_b)),   old(cipher_num_bytes));

//        // TODO: First two clauses work around Vale's type limitations
        4096 * length(cipher) < pow2_32 /\
        4096 * length(auth) < pow2_32 /\
        plain == gcm_decrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), cipher, auth, le_quad32_to_bytes(buffer128_read(tag_b, 0, old(mem))))._1 /\
        (rax = 0) == gcm_decrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), cipher, auth, le_quad32_to_bytes(buffer128_read(tag_b, 0, old(mem))))._2;
{
    gcm_decrypt_stdcall_inner_inner(alg, args_b, iv_BE, cipher_b, auth_b, iv_b, out_b, tag_b, key, round_keys, keys_b);

    assert validDstAddrs128(mem, r15, tag_b, 1, memTaint, Secret);

    // Auth tag is in xmm1; compare it to the value in memory
    Load128_buffer(xmm0, r15, 0, Secret, tag_b, 0);
    assert xmm0 == buffer128_read(tag_b, 0, mem);       // OBSERVE?
    assert xmm0 == buffer128_read(tag_b, 0, old(mem));  // OBSERVE?
    ghost var alleged_tag_quad := xmm0;
    ghost var computed_tag := xmm1;

    XmmEqual();
    ghost var auth   := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)),  old(auth_num_bytes));
    ghost var cipher  := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), cipher_b)), old(cipher_num_bytes));
    decrypt_helper(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), cipher, auth, rax, alleged_tag_quad, computed_tag);
}

#reset-options "--z3rlimit 200"
procedure {:quick} gcm_decrypt_stdcall(
    inline win:bool,
    inline alg:algorithm,
    ghost args_b:buffer64,
    ghost iv_BE:quad32,

    ghost cipher_b:buffer128,
    ghost auth_b:buffer128,
    ghost iv_b:buffer128,
    ghost out_b:buffer128,
    ghost tag_b:buffer128,

    ghost key:seq(nat32),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    reads memTaint;
    modifies
        rax; rbx; rcx; rdx; r8; r9; r10; r11; r12; r13; r14; r15; 
        xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; xmm6; xmm7; xmm8; xmm9; xmm10; xmm11; xmm12; xmm13; xmm14; xmm15;
        rbp; rdi; rsi;
        mem; efl; rsp; stack;
    lets
        args_ptr := if win then rcx else rdi;

        cipher_ptr         := buffer64_read(args_b, 0, mem);
        cipher_num_bytes   := buffer64_read(args_b, 1, mem);
        auth_ptr          := buffer64_read(args_b, 2, mem);
        auth_num_bytes    := buffer64_read(args_b, 3, mem);
        iv_ptr            := buffer64_read(args_b, 4, mem);
        expanded_key_ptr  := buffer64_read(args_b, 5, mem);
        out_ptr           := buffer64_read(args_b, 6, mem);
        tag_ptr           := buffer64_read(args_b, 7, mem);

    requires
        rsp == init_rsp(stack);
        validSrcAddrs64(mem, args_ptr, args_b, 8, memTaint, Public);

        buffers_disjoint128(cipher_b, out_b);
        buffers_disjoint128(auth_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        buffers_disjoint128(out_b, tag_b);

        validSrcAddrs128(mem, cipher_ptr, cipher_b, bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);
        validSrcAddrs128(mem, auth_ptr, auth_b, bytes_to_quad_size(auth_num_bytes), memTaint, Secret);
        validSrcAddrs128(mem, iv_ptr, iv_b, 1, memTaint, Secret);
        validDstAddrs128(mem, out_ptr, out_b, bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);
        validDstAddrs128(mem, tag_ptr, tag_b, 1, memTaint, Secret);

        cipher_ptr + 16 * bytes_to_quad_size(cipher_num_bytes) < pow2_64;
        auth_ptr  + 16 * bytes_to_quad_size(auth_num_bytes)  < pow2_64;
        out_ptr   + 16 * bytes_to_quad_size(cipher_num_bytes) < pow2_64;
        buffer_length(cipher_b) == buffer_length(out_b);
        buffer_length(out_b) == bytes_to_quad_size(cipher_num_bytes);
        buffer_length(auth_b) == bytes_to_quad_size(auth_num_bytes);
        256 * buffer_length(cipher_b) < pow2_32;
        4096 * cipher_num_bytes < pow2_32;
        4096 * auth_num_bytes < pow2_32;

        iv_BE == reverse_bytes_quad32(buffer128_read(iv_b, 0, mem));

        // To simplify length calculations, restrict auth and cipher length further
        256 * bytes_to_quad_size(auth_num_bytes)  < pow2_32;
        256 * bytes_to_quad_size(cipher_num_bytes) < pow2_32;

        // AES reqs
        aesni_enabled;
        alg = AES_128 || alg = AES_256;
        is_aes_key_LE(alg, key);
        length(round_keys) == nr(alg) + 1;
        round_keys == key_to_round_keys_LE(alg, key);
        validSrcAddrs128(mem, expanded_key_ptr, keys_b, nr(alg) + 1, memTaint, Secret);
        buffer128_as_seq(mem, keys_b) == round_keys;

        pclmulqdq_enabled;

    ensures
        modifies_mem(loc_buffer(out_b), old(mem), mem);

        validSrcAddrs128(mem, old(out_ptr), out_b, bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);

        let auth   := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)),  old(auth_num_bytes));
        let cipher  := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), cipher_b)), old(cipher_num_bytes));
        let plain := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem,      out_b)),   old(cipher_num_bytes));

//        // TODO: First two clauses work around Vale's type limitations
        4096 * length(cipher) < pow2_32 /\
        4096 * length(auth) < pow2_32 /\
        plain == gcm_decrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), cipher, auth, le_quad32_to_bytes(buffer128_read(tag_b, 0, old(mem))))._1 /\
        (rax = 0) == gcm_decrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(iv_BE), cipher, auth, le_quad32_to_bytes(buffer128_read(tag_b, 0, old(mem))))._2;

        // Calling convention for caller/callee saved registers
        rsp == old(rsp);

        // Windows:
        win ==> rbx == old(rbx);
        win ==> rbp == old(rbp);
        win ==> rdi == old(rdi);
        win ==> rsi == old(rsi);
        win ==> r12 == old(r12);
        win ==> r13 == old(r13);
        win ==> r14 == old(r14);
        win ==> r15 == old(r15);

        win ==> xmm6  == old(xmm6);
        win ==> xmm7  == old(xmm7);
        win ==> xmm8  == old(xmm8);
        win ==> xmm9  == old(xmm9);
        win ==> xmm10 == old(xmm10);
        win ==> xmm11 == old(xmm11);
        win ==> xmm12 == old(xmm12);
        win ==> xmm13 == old(xmm13);
        win ==> xmm14 == old(xmm14);
        win ==> xmm15 == old(xmm15);

        // Linux:
        !win ==> rbx == old(rbx);
        !win ==> rbp == old(rbp);
        !win ==> r12 == old(r12);
        !win ==> r13 == old(r13);
        !win ==> r14 == old(r14);
        !win ==> r15 == old(r15);
{
    // Shuffle the incoming pointer around
    inline if (win) {
        Mov64(r9, rcx);
    } else {
        Mov64(r9, rdi);
    }

    Push(r15);
    Push(r14);
    Push(r13);
    Push(r12);
    Push(rsi);
    Push(rdi);
    Push(rbp);
    Push(rbx);

    inline if (win) {
        PushXmm(xmm15, rax);
        PushXmm(xmm14, rax);
        PushXmm(xmm13, rax);
        PushXmm(xmm12, rax);
        PushXmm(xmm11, rax);
        PushXmm(xmm10, rax);
        PushXmm(xmm9,  rax);
        PushXmm(xmm8,  rax);
        PushXmm(xmm7,  rax);
        PushXmm(xmm6,  rax);
    }

    gcm_decrypt_stdcall_inner(alg, args_b, iv_BE, cipher_b, auth_b, iv_b, out_b, tag_b, key, round_keys, keys_b);
    Mov64(rdx, rax);        // Save a copy of rax == result

    inline if (win) {
        PopXmm(xmm6,  rax, old(xmm6));
        PopXmm(xmm7,  rax, old(xmm7));
        PopXmm(xmm8,  rax, old(xmm8));
        PopXmm(xmm9,  rax, old(xmm9));
        PopXmm(xmm10, rax, old(xmm10));
        PopXmm(xmm11, rax, old(xmm11));
        PopXmm(xmm12, rax, old(xmm12));
        PopXmm(xmm13, rax, old(xmm13));
        PopXmm(xmm14, rax, old(xmm14));
        PopXmm(xmm15, rax, old(xmm15));
    }

    Pop(rbx);
    Pop(rbp);
    Pop(rdi);
    Pop(rsi);
    Pop(r12);
    Pop(r13);
    Pop(r14);
    Pop(r15);

    Mov64(rax, rdx);
}

#verbatim

#reset-options "--z3rlimit 200"
#endverbatim

// Take arguments in registers and the stack, instead of via an in-memory struct
procedure {:quick}{:public}{:exportSpecs} gcm_decrypt2_stdcall(
    inline win:bool,
    inline alg:algorithm,

    ghost cipher_b:buffer128,
    ghost cipher_num_bytes:nat64,
    ghost auth_b:buffer128,
    ghost auth_num_bytes:nat64,
    ghost iv_b:buffer128,
    ghost keys_b:buffer128,    
    ghost out_b:buffer128,
    ghost tag_b:buffer128,

    ghost key:seq(nat32)
    )
    reads memTaint;
    modifies
        rax; rbx; rcx; rdx; rdi; rsi; rsp; rbp; r8; r9; r10; r11; r12; r13; r14; r15; 
        xmm0; xmm1; xmm2; xmm3; xmm4; xmm5; xmm6; xmm7; xmm8; xmm9; xmm10; xmm11; xmm12; xmm13; xmm14; xmm15;
        mem; efl; stack;
    lets
        cipher_ptr       := if win then rcx else rdi;
        auth_ptr         := if win then r8  else rdx;
        iv_ptr           := if win then load_stack64(rsp + 32 + 8 + 0, stack) else r8;
        expanded_key_ptr := if win then load_stack64(rsp + 32 + 8 + 8, stack) else r9;
        out_ptr          := if win then load_stack64(rsp + 32 + 8 + 16, stack) else load_stack64(rsp + 8 + 0, stack);
        tag_ptr          := if win then load_stack64(rsp + 32 + 8 + 24, stack) else load_stack64(rsp + 8 + 8, stack);

    requires
        buffers_disjoint128(cipher_b, out_b);
        buffers_disjoint128(auth_b, out_b);
        buffers_disjoint128(keys_b, out_b);
        buffers_disjoint128(out_b, tag_b);

        rsp == init_rsp(stack);
        win ==> valid_src_stack64(rsp + 32 + 8 + 0, stack);
        win ==> valid_src_stack64(rsp + 32 + 8 + 8, stack);
        win ==> valid_src_stack64(rsp + 32 + 8 + 16, stack);
        win ==> valid_src_stack64(rsp + 32 + 8 + 24, stack);
        !win ==> valid_src_stack64(rsp + 8 + 0, stack);
        !win ==> valid_src_stack64(rsp + 8 + 8, stack);        

        cipher_num_bytes == (if win then rdx else rsi);
        auth_num_bytes == (if win then r9 else rcx);
        validSrcAddrs128(mem, cipher_ptr, cipher_b, bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);
        validSrcAddrs128(mem, auth_ptr, auth_b, bytes_to_quad_size(auth_num_bytes), memTaint, Secret);
        validSrcAddrs128(mem, iv_ptr, iv_b, 1, memTaint, Secret);
        validDstAddrs128(mem, out_ptr, out_b, bytes_to_quad_size(cipher_num_bytes), memTaint, Secret);
        validDstAddrs128(mem, tag_ptr, tag_b, 1, memTaint, Secret);

        cipher_ptr + 16 * bytes_to_quad_size(cipher_num_bytes) < pow2_64;
        auth_ptr  + 16 * bytes_to_quad_size(auth_num_bytes)  < pow2_64;
        out_ptr   + 16 * bytes_to_quad_size(cipher_num_bytes) < pow2_64;
        buffer_length(cipher_b) == buffer_length(out_b);
        buffer_length(out_b) == bytes_to_quad_size(cipher_num_bytes);
        buffer_length(auth_b) == bytes_to_quad_size(auth_num_bytes);
        256 * buffer_length(cipher_b) < pow2_32;
        4096 * cipher_num_bytes < pow2_32;
        4096 * auth_num_bytes < pow2_32;

        // To simplify length calculations, restrict auth and plain length further
        256 * bytes_to_quad_size(auth_num_bytes)  < pow2_32;
        256 * bytes_to_quad_size(cipher_num_bytes) < pow2_32;

        // AES reqs
        aesni_enabled;
        alg = AES_128 || alg = AES_256;
        is_aes_key_LE(alg, key);
        buffer128_as_seq(mem, keys_b) == key_to_round_keys_LE(alg, key);
        validSrcAddrs128(mem, expanded_key_ptr, keys_b, nr(alg) + 1, memTaint, Secret);

        pclmulqdq_enabled;
    ensures
        modifies_mem(loc_buffer(out_b), old(mem), mem);

        validSrcAddrs128(mem, old(out_ptr), out_b, old(bytes_to_quad_size(cipher_num_bytes)), memTaint, Secret);
        validSrcAddrs128(mem, old(tag_ptr), tag_b, 1, memTaint, Secret);

        let auth   := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)),  old(auth_num_bytes));
        let cipher  := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), cipher_b)), old(cipher_num_bytes));
        let plain := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem,      out_b)),   old(cipher_num_bytes));
        let expected_tag := le_quad32_to_bytes(buffer128_read(tag_b, 0, old(mem)));

//        // TODO: First two clauses work around Vale's type limitations
        4096 * length(cipher) < pow2_32 /\
        4096 * length(auth) < pow2_32 /\
        plain == gcm_decrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(reverse_bytes_quad32(buffer128_read(iv_b, 0, old(mem)))), cipher, auth, expected_tag)._1 /\
        (rax = 0) == gcm_decrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(reverse_bytes_quad32(buffer128_read(iv_b, 0, old(mem)))), cipher, auth, expected_tag)._2;

        // Calling convention for caller/callee saved registers
        rsp == old(rsp);

        // Windows:
        win ==> rbx == old(rbx);
        win ==> rbp == old(rbp);
        win ==> rdi == old(rdi);
        win ==> rsi == old(rsi);
        win ==> r12 == old(r12);
        win ==> r13 == old(r13);
        win ==> r14 == old(r14);
        win ==> r15 == old(r15);

        win ==> xmm6  == old(xmm6);
        win ==> xmm7  == old(xmm7);
        win ==> xmm8  == old(xmm8);
        win ==> xmm9  == old(xmm9);
        win ==> xmm10 == old(xmm10);
        win ==> xmm11 == old(xmm11);
        win ==> xmm12 == old(xmm12);
        win ==> xmm13 == old(xmm13);
        win ==> xmm14 == old(xmm14);
        win ==> xmm15 == old(xmm15);

        // Linux:
        !win ==> rbx == old(rbx);
        !win ==> rbp == old(rbp);
        !win ==> r12 == old(r12);
        !win ==> r13 == old(r13);
        !win ==> r14 == old(r14);
        !win ==> r15 == old(r15);
{
    Push(r15);
    Push(r14);
    Push(r13);
    Push(r12);
    Push(rsi);
    Push(rdi);
    Push(rbp);
    Push(rbx);

    inline if (win) {
        PushXmm(xmm15, rax);
        PushXmm(xmm14, rax);
        PushXmm(xmm13, rax);
        PushXmm(xmm12, rax);
        PushXmm(xmm11, rax);
        PushXmm(xmm10, rax);
        PushXmm(xmm9,  rax);
        PushXmm(xmm8,  rax);
        PushXmm(xmm7,  rax);
        PushXmm(xmm6,  rax);
    }


    // Shuffle the incoming arguments around
    inline if (win) {
        Mov64(r14, rcx);
        Mov64(r13, rdx);
        Mov64(rax, r8);
        Mov64(r11, r9);

        // These are offset, since we already moved rsp during callee_save_registers
        Load64_stack(r10, rsp, 224 + 40 + 0);
        Load64_stack(r8,  rsp, 224 + 40 + 8);
        Load64_stack(rbx, rsp, 224 + 40 +16);
        Load64_stack(r15, rsp, 224 + 40 +24);
    } else {
        Mov64(r14, rdi);
        Mov64(r13, rsi);
        Mov64(rax, rdx);
        Mov64(r11, rcx);
        Mov64(r10, r8);
        Mov64(r8,  r9);

        // These are offset, since we already moved rsp during callee_save_registers
        Load64_stack(rbx, rsp, 64 + 8 + 0);
        Load64_stack(r15, rsp, 64 + 8 + 8);
    }

    // Load the IV into its XMM register
    Load128_buffer(xmm7, r10, 0, Secret, iv_b, 0);

    gcm_core(alg, reverse_bytes_quad32(buffer128_read(iv_b, 0, old(mem))), cipher_b, auth_b, out_b, key, buffer128_as_seq(old(mem), keys_b), keys_b);

    ghost var auth   := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), auth_b)),   old(auth_num_bytes));
    ghost var cipher := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(old(mem), cipher_b)), old(cipher_num_bytes));
    ghost var plain  := slice_work_around(le_seq_quad32_to_bytes(buffer128_as_seq(mem,      out_b)),    old(cipher_num_bytes));
    ghost var alleged_tag := le_quad32_to_bytes(buffer128_read(tag_b, 0, old(mem)));
    lemma_gcm_encrypt_decrypt_equiv(alg, key, reverse_bytes_quad32(buffer128_read(iv_b, 0, old(mem))), plain, cipher, auth, alleged_tag);

    // Auth tag is in xmm1; compare it to the value in memory
    Load128_buffer(xmm0, r15, 0, Secret, tag_b, 0);
    assert xmm0 == buffer128_read(tag_b, 0, mem);       // OBSERVE?
    assert xmm0 == buffer128_read(tag_b, 0, old(mem));  // OBSERVE?
    ghost var alleged_tag_quad := xmm0;
    ghost var computed_tag := xmm1;

    XmmEqual();
    decrypt_helper(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(reverse_bytes_quad32(buffer128_read(iv_b, 0, old(mem)))), cipher, auth, rax, buffer128_read(tag_b, 0, old(mem)), computed_tag);

    assert (
        is_aes_key_LE(alg, key) /\
        4096 * length(cipher) < pow2_32 /\
        4096 * length(auth) < pow2_32 /\
           (rax = 0) == gcm_decrypt_LE(alg, seq_nat32_to_seq_nat8_LE(key), be_quad32_to_bytes(reverse_bytes_quad32(buffer128_read(iv_b, 0, old(mem)))), cipher, auth, le_quad32_to_bytes(buffer128_read(tag_b, 0, old(mem))))._2);

    inline if (win) {
        PopXmm(xmm6,  rdx, old(xmm6));
        PopXmm(xmm7,  rdx, old(xmm7));
        PopXmm(xmm8,  rdx, old(xmm8));
        PopXmm(xmm9,  rdx, old(xmm9));
        PopXmm(xmm10, rdx, old(xmm10));
        PopXmm(xmm11, rdx, old(xmm11));
        PopXmm(xmm12, rdx, old(xmm12));
        PopXmm(xmm13, rdx, old(xmm13));
        PopXmm(xmm14, rdx, old(xmm14));
        PopXmm(xmm15, rdx, old(xmm15));
    }

    Pop(rbx);
    Pop(rbp);
    Pop(rdi);
    Pop(rsi);
    Pop(r12);
    Pop(r13);
    Pop(r14);
    Pop(r15);


}
