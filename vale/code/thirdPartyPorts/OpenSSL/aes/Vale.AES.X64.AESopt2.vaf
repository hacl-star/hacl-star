include "../../../arch/x64/Vale.X64.InsBasic.vaf"
include "../../../arch/x64/Vale.X64.InsMem.vaf"
include "../../../arch/x64/Vale.X64.InsVector.vaf"
include "../../../arch/x64/Vale.X64.InsAes.vaf"
include "../../../crypto/aes/x64/Vale.AES.X64.PolyOps.vaf"
include{:fstar}{:open} "Vale.Def.Prop_s"
include{:fstar}{:open} "Vale.Def.Opaque_s"
include{:fstar}{:open} "Vale.Def.Words_s"
include{:fstar}{:open} "Vale.Def.Types_s"
include{:/*TODO*/fstar}{:open} "FStar.Seq.Base"
include{:fstar}{:open} "Vale.AES.AES_s"
include{:fstar}{:open} "Vale.X64.Machine_s"
include{:fstar}{:open} "Vale.X64.Memory"
include{:fstar}{:open} "Vale.X64.State"
include{:fstar}{:open} "Vale.X64.Decls"
include{:fstar}{:open} "Vale.X64.QuickCode"
include{:fstar}{:open} "Vale.X64.QuickCodes"
include{:fstar}{:open} "Vale.Arch.Types"
include{:fstar}{:open} "Vale.AES.AES_helpers"
include{:fstar}{:open} "Vale.Poly1305.Math"
include{:fstar}{:open} "Vale.AES.GCM_helpers"
include{:fstar}{:open} "Vale.AES.GCTR_s"
include{:fstar}{:open} "Vale.AES.GCTR"
include{:fstar}{:open} "Vale.Arch.TypesNative"
include{:fstar}{:open} "Vale.X64.CPU_Features_s"

include{:fstar}{:open} "Vale.Math.Poly2_s"
include{:fstar}{:open} "Vale.Math.Poly2"
include{:fstar}{:open} "Vale.Math.Poly2.Bits_s"
include{:fstar}{:open} "Vale.Math.Poly2.Bits"
include{:fstar}{:open} "Vale.Math.Poly2.Words"
include{:fstar}{:open} "Vale.Math.Poly2.Lemmas"
include{:fstar}{:open} "Vale.AES.GF128_s"
include{:fstar}{:open} "Vale.AES.GF128"
include{:fstar}{:open} "Vale.AES.GHash"
module Vale.AES.X64.AESopt2

#verbatim{:interface}{:implementation}
open Vale.Def.Prop_s
open Vale.Def.Opaque_s
open Vale.Def.Words_s
open Vale.Def.Types_s
open FStar.Seq
open Vale.AES.AES_s
open Vale.X64.Machine_s
open Vale.X64.Memory
open Vale.X64.State
open Vale.X64.Decls
open Vale.X64.InsBasic
open Vale.X64.InsMem
open Vale.X64.InsVector
open Vale.X64.InsAes
open Vale.X64.QuickCode
open Vale.X64.QuickCodes
open Vale.Arch.Types
open Vale.AES.AES_helpers
open Vale.Poly1305.Math    // For lemma_poly_bits64()
open Vale.AES.GCM_helpers
open Vale.AES.GCTR_s
open Vale.AES.GCTR
open Vale.Arch.TypesNative
open Vale.X64.CPU_Features_s
open Vale.AES.X64.PolyOps

open Vale.Math.Poly2_s
open Vale.Math.Poly2
open Vale.Math.Poly2.Bits_s
open Vale.Math.Poly2.Bits
open Vale.Math.Poly2.Lemmas
open Vale.AES.GF128_s
open Vale.AES.GF128
open Vale.AES.GHash
#endverbatim

#token +. precedence +
#token *. precedence *
#token %. precedence *
#token ~~ precedence !
function operator(+.) (a:poly, b:poly):poly := add;
function operator(*.) (a:poly, b:poly):poly := mul;
function operator(%.) (a:poly, b:poly):poly := mod;
function operator(~~) (a:quad32):poly := of_quad32;
function operator([]) #[a:Type(0)](s:FStar.Seq.Base.seq(a), i:int):a extern;
#verbatim{:interface}
let va_subscript_FStar__Seq__Base__seq = Seq.index
#endverbatim

procedure MulAdd_unroll(
        ghost hkeys_b:buffer128,
        ghost scratch_b:buffer128,
        ghost h:poly,
        ghost prev:poly,
        ghost data:seq(quad32))
    {:quick}
    lets
        Xip @= r9;
        Ii @= xmm0; T1 @= xmm1; T2 @= xmm2; Hkey @= xmm3;
        Z0 @= xmm4; Z1 @= xmm5; Z2 @= xmm6; Z3 @= xmm7;
        Xi @= xmm8;
        pdata := fun_seq_quad32_LE_poly128(data);
    reads
        mem; memTaint;
        Xip; rbp;
    modifies
        efl;
        rax;
        Ii; T1; T2; Hkey; Z0; Z1; Z2; Z3; Xi;
    requires
        pclmulqdq_enabled && avx_enabled;
        length(data) == 6;
        Z3 == reverse_bytes_quad32(data[5]);
        buffers_disjoint128(hkeys_b, scratch_b);
        validSrcAddrs128(mem, Xip - 0x20, hkeys_b, 8, memTaint, Secret);
        of_quad32(buffer128_read(hkeys_b, 0, mem)) == gf128_power(h, 1);
        of_quad32(buffer128_read(hkeys_b, 1, mem)) == gf128_power(h, 2);
        of_quad32(buffer128_read(hkeys_b, 3, mem)) == gf128_power(h, 3);
        of_quad32(buffer128_read(hkeys_b, 4, mem)) == gf128_power(h, 4);
        of_quad32(buffer128_read(hkeys_b, 6, mem)) == gf128_power(h, 5);
        of_quad32(buffer128_read(hkeys_b, 7, mem)) == gf128_power(h, 6);
        validSrcAddrs128(mem, rbp, scratch_b, 8, memTaint, Secret);
        of_quad32(Xi) +. of_quad32(Z0) +. of_quad32(buffer128_read(scratch_b, 1, mem)) == prev;
        buffer128_read(scratch_b, 3, mem) == reverse_bytes_quad32(data[4]);
        buffer128_read(scratch_b, 4, mem) == reverse_bytes_quad32(data[3]);
        buffer128_read(scratch_b, 5, mem) == reverse_bytes_quad32(data[2]);
        buffer128_read(scratch_b, 6, mem) == reverse_bytes_quad32(data[1]);
        buffer128_read(scratch_b, 7, mem) == reverse_bytes_quad32(data[0]);
    ensures
        of_quad32(Z0) +. shift(of_quad32(Z2), 64) +. shift(of_quad32(Z3), 128) ==
            ghash_unroll_back(h, prev, pdata, 0, 6, 5);
        old(buffer128_read(scratch_b, 2, mem)) == buffer128_read(scratch_b, 2, mem);
        Hkey == Mkfour(0, 0, 0, 0xc2000000);
{
    let data0 := #poly(pdata(0));
    let data1 := #poly(pdata(1));
    let data2 := #poly(pdata(2));
    let data3 := #poly(pdata(3));
    let data4 := #poly(pdata(4));
    let data5 := #poly(pdata(5));

    // 1
    Load128_buffer(Hkey, Xip, (-0x20), Secret, hkeys_b, 0);
    VPolyMul(T1, Z3, Hkey, false, false);
    VPolyMul(Z1, Z3, Hkey, false, true);
    Load128_buffer(Ii, rbp, 0x30, Secret, scratch_b, 3);
    VPolyMul(Z2, Z3, Hkey, true, false);
    VPolyMul(Z3, Z3, Hkey, true, true);
    lemma_Mul128(data5, gf128_power(h, 1));
    ghost var z := data5 *. gf128_power(h, 1);
    assert z == ghash_unroll_back(h, prev, pdata, 0, 6, 0);
    // assert z == ~~T1 +. shift(~~Z1 +. ~~Z2, 64) +. shift(~~Z3, 128);
    lemma_add_commute(~~Z1, ~~Z2);
    // assert z == ~~T1 +. shift(~~Z2 +. ~~Z1, 64) +. shift(~~Z3, 128);

    Load128_buffer(Hkey, Xip, (-0x10), Secret, hkeys_b, 1);
    VPolyAdd(Z2, Z2, Z1);
    let z0 := ~~T1;
    // assert z == z0 +. shift(~~Z2, 64) +. shift(~~Z3, 128);

    // 2
    VPolyMul(Z1, Ii, Hkey, false, false);
    VPolyAdd(Xi, Xi, Z0);
    VPolyAdd(Z0, T1, Z1);
    VPolyMul(T1, Ii, Hkey, false, true);
    VPolyMul(T2, Ii, Hkey, true, false);
    VPolyAdd(Xi, Xi, Mem128(rbp, 0x10, Secret, scratch_b, 1));
    // assert ~~Xi == prev;
    VPolyMul(Hkey, Ii, Hkey, true, true);
    lemma_Mul128_accum(z0, ~~Z2, ~~Z3, data4, gf128_power(h, 2));
    z := z +. data4 *. gf128_power(h, 2);
    assert z == ghash_unroll_back(h, prev, pdata, 0, 6, 1);
    // assert z == ~~Z0 +. shift(~~Z2 +. ~~T1 +. ~~T2, 64) +. shift(~~Z3 +. ~~Hkey, 128);

    Load128_buffer(Ii, rbp, 0x40, Secret, scratch_b, 4);
    Load128_buffer(Z1, Xip, 0x10, Secret, hkeys_b, 3);
    VPolyAdd(Z2, Z2, T1);
    // assert z == ~~Z0 +. shift(~~Z2 +. ~~T2, 64) +. shift(~~Z3 +. ~~Hkey, 128);

    // 3
    VPolyMul(T1, Ii, Z1, false, false);
    VPolyAdd(Z2, Z2, T2);
    // assert z == ~~Z0 +. shift(~~Z2, 64) +. shift(~~Z3 +. ~~Hkey, 128);
    VPolyMul(T2, Ii, Z1, false, true);
    VPolyAdd(Z3, Z3, Hkey);
    // assert z == ~~Z0 +. shift(~~Z2, 64) +. shift(~~Z3, 128);
    VPolyMul(Hkey, Ii, Z1, true, false);
    VPolyMul(Z1, Ii, Z1, true, true);
    lemma_Mul128_accum(~~Z0, ~~Z2, ~~Z3, data3, gf128_power(h, 3));
    z := z +. data3 *. gf128_power(h, 3);
    assert z == ghash_unroll_back(h, prev, pdata, 0, 6, 2);
    // assert z == (~~Z0 +. ~~T1) +. shift(~~Z2 +. ~~T2 +. ~~Hkey, 64) +. shift(~~Z3 +. ~~Z1, 128);

    Load128_buffer(Ii, rbp, 0x50, Secret, scratch_b, 5);
    VPolyAdd(Z0, Z0, T1);
    // assert z == ~~Z0 +. shift(~~Z2 +. ~~T2 +. ~~Hkey, 64) +. shift(~~Z3 +. ~~Z1, 128);
    Load128_buffer(T1, Xip, 0x20, Secret, hkeys_b, 4);
    VPolyAdd(Z2, Z2, T2);
    // assert z == ~~Z0 +. shift(~~Z2 +. ~~Hkey, 64) +. shift(~~Z3 +. ~~Z1, 128);

    // 4
    VPolyMul(T2, Ii, T1, false, false);
    VPolyAdd(Z2, Z2, Hkey);
    // assert z == ~~Z0 +. shift(~~Z2, 64) +. shift(~~Z3 +. ~~Z1, 128);
    VPolyMul(Hkey, Ii, T1, false, true);
    VPolyAdd(Z3, Z3, Z1);
    // assert z == ~~Z0 +. shift(~~Z2, 64) +. shift(~~Z3, 128);
    VPolyMul(Z1, Ii, T1, true, false);
    VPolyMul(T1, Ii, T1, true, true);
    lemma_Mul128_accum(~~Z0, ~~Z2, ~~Z3, data2, gf128_power(h, 4));
    z := z +. data2 *. gf128_power(h, 4);
    assert z == ghash_unroll_back(h, prev, pdata, 0, 6, 3);
    // assert z == (~~Z0 +. ~~T2) +. shift(~~Z2 +. ~~Hkey +. ~~Z1, 64) +. shift(~~Z3 +. ~~T1, 128);

    Load128_buffer(Ii, rbp, 0x60, Secret, scratch_b, 6);
    VPolyAdd(Z0, Z0, T2);
    // assert z == ~~Z0 +. shift(~~Z2 +. ~~Hkey +. ~~Z1, 64) +. shift(~~Z3 +. ~~T1, 128);
    Load128_buffer(T2, Xip, 0x40, Secret, hkeys_b, 6);
    VPolyAdd(Z2, Z2, Hkey);
    // assert z == ~~Z0 +. shift(~~Z2 +. ~~Z1, 64) +. shift(~~Z3 +. ~~T1, 128);

    // 5
    VPolyMul(Hkey, Ii, T2, false, false);
    VPolyAdd(Z2, Z2, Z1);
    // assert z == ~~Z0 +. shift(~~Z2, 64) +. shift(~~Z3 +. ~~T1, 128);
    VPolyMul(Z1, Ii, T2, false, true);
    VPolyAdd(Z3, Z3, T1);
    // assert z == ~~Z0 +. shift(~~Z2, 64) +. shift(~~Z3, 128);
    VPolyMul(T1, Ii, T2, true, false);
    VPolyAdd(Xi, Xi, Mem128(rbp, 0x70, Secret, scratch_b, 7));
    VPolyMul(T2, Ii, T2, true, true);
    lemma_Mul128_accum(~~Z0, ~~Z2, ~~Z3, data1, gf128_power(h, 5));
    z := z +. data1 *. gf128_power(h, 5);
    assert z == ghash_unroll_back(h, prev, pdata, 0, 6, 4);
    // assert z == (~~Z0 +. ~~Hkey) +. shift(~~Z2 +. ~~Z1 +. ~~T1, 64) +. shift(~~Z3 +. ~~T2, 128);

    VPolyAdd(Z0, Z0, Hkey);
    // assert z == ~~Z0 +. shift(~~Z2 +. ~~Z1 +. ~~T1, 64) +. shift(~~Z3 +. ~~T2, 128);
    Load128_buffer(Hkey, Xip, 0x50, Secret, hkeys_b, 7);
    VPolyAdd(Z2, Z2, Z1);
    // assert z == ~~Z0 +. shift(~~Z2 +. ~~T1, 64) +. shift(~~Z3 +. ~~T2, 128);

    // 6
    VPolyMul(Z1, Xi, Hkey, false, true);
    VPolyAdd(Z2, Z2, T1);
    // assert z == ~~Z0 +. shift(~~Z2, 64) +. shift(~~Z3 +. ~~T2, 128);
    VPolyMul(T1, Xi, Hkey, true, false);
    VPolyAdd(Z3, Z3, T2);
    // assert z == ~~Z0 +. shift(~~Z2, 64) +. shift(~~Z3, 128);
    VPolyMul(T2, Xi, Hkey, false, false);
    VPolyMul(Xi, Xi, Hkey, true, true);
    lemma_Mul128_accum(~~Z0, ~~Z2, ~~Z3, prev +. data0, gf128_power(h, 6));
    z := z +. (prev +. data0) *. gf128_power(h, 6);
    assert z == ghash_unroll_back(h, prev, pdata, 0, 6, 5);
    // assert z == (~~Z0 +. ~~T2) +. shift(~~Z2 +. ~~Z1 +. ~~T1, 64) +. shift(~~Z3 +. ~~Xi, 128);

    VPolyAdd(Z2, Z2, Z1);
    // assert z == (~~Z0 +. ~~T2) +. shift(~~Z2 +. ~~T1, 64) +. shift(~~Z3 +. ~~Xi, 128);
    VPolyAdd(Z2, Z2, T1);
    // assert z == (~~Z0 +. ~~T2) +. shift(~~Z2, 64) +. shift(~~Z3 +. ~~Xi, 128);
    // vpslldq in Reduce
    VPolyAdd(Z0, Z0, T2);
    // assert z == ~~Z0 +. shift(~~Z2, 64) +. shift(~~Z3 +. ~~Xi, 128);
    ZeroXmm(Hkey);
    PinsrdImm(Hkey, 0xc2000000, 3, rax); // REVIEW: vmovdqu into Hkey
    VPolyAdd(Z3, Z3, Xi);

    // assert z == ~~Z0 +. shift(~~Z2, 64) +. shift(~~Z3, 128);
}

procedure Reduce(ghost f:poly)
    {:quick}
    lets
        Ii @= xmm0; Hkey @= xmm3;
        Z0 @= xmm4; Z1 @= xmm5; Z2 @= xmm6; Z3 @= xmm7;
        Xi @= xmm8;
        g := monomial(128) +. f;
        c := reverse(shift(f, (-1)), 63);
        a0 := of_quad32(Z0);
        a1 := of_quad32(Z2);
        a2 := of_quad32(Z3);
        a := a0 +. shift(a1, 64) +. shift(a2, 128);
    modifies
        efl;
        Ii; Hkey; Z0; Z1; Z2; Z3; Xi;
    requires
        pclmulqdq_enabled && avx_enabled;
        shift(of_quad32(Hkey), (-64)) == c;
        degree(f) < 64;
        degree(g) == 128;
        poly_index(f, 0);
    ensures
        of_quad32(Xi) == reverse(reverse(a, 255) %. g, 127);
{
    VLow64ToHigh(Z1, Z2);
    VPolyAdd(Z0, Z0, Z1);
    VSwap(Ii, Z0);
    VPolyMul(Z0, Z0, Hkey, false, true);
    VHigh64ToLow(Z2, Z2);
    VPolyAdd(Z3, Z3, Z2);
    VPolyAdd(Z0, Ii, Z0);
    // TODO: save Z3 in memory
    VSwap(Xi, Z0);
    VPolyMul(Z0, Z0, Hkey, false, true);
    VPolyAdd(Xi, Xi, Z3);
    VPolyAdd(Xi, Xi, Z0);
    lemma_reduce_rev(a0, a1, a2, f, 64);
}

procedure GhashUnroll6x(
        ghost hkeys_b:buffer128,
        ghost scratch_b:buffer128,
        ghost h_LE:quad32,
        ghost y_prev:quad32,
        ghost data:seq(quad32))
    {:public}
    {:quick}
    lets
        Xip @= r9;
        Ii @= xmm0; T1 @= xmm1; T2 @= xmm2; Hkey @= xmm3;
        Z0 @= xmm4; Z1 @= xmm5; Z2 @= xmm6; Z3 @= xmm7;
        Xi @= xmm8;
        h := of_quad32(reverse_bytes_quad32(h_LE));
        prev := of_quad32(reverse_bytes_quad32(y_prev));
        pdata := fun_seq_quad32_LE_poly128(data);
    reads
        mem; memTaint;
        Xip; rbp;
    modifies
        efl;
        rax;
        Ii; T1; T2; Hkey; Z0; Z1; Z2; Z3; Xi;
    requires
        pclmulqdq_enabled && avx_enabled;
        length(data) == 6;
        Z3 == reverse_bytes_quad32(data[5]);
        buffers_disjoint128(hkeys_b, scratch_b);
        validSrcAddrs128(mem, Xip - 0x20, hkeys_b, 8, memTaint, Secret);
        of_quad32(buffer128_read(hkeys_b, 0, mem)) == gf128_power(h, 1);
        of_quad32(buffer128_read(hkeys_b, 1, mem)) == gf128_power(h, 2);
        of_quad32(buffer128_read(hkeys_b, 3, mem)) == gf128_power(h, 3);
        of_quad32(buffer128_read(hkeys_b, 4, mem)) == gf128_power(h, 4);
        of_quad32(buffer128_read(hkeys_b, 6, mem)) == gf128_power(h, 5);
        of_quad32(buffer128_read(hkeys_b, 7, mem)) == gf128_power(h, 6);
        validSrcAddrs128(mem, rbp, scratch_b, 8, memTaint, Secret);
        of_quad32(Xi) +. of_quad32(Z0) +. of_quad32(buffer128_read(scratch_b, 1, mem)) == prev;
        buffer128_read(scratch_b, 3, mem) == reverse_bytes_quad32(data[4]);
        buffer128_read(scratch_b, 4, mem) == reverse_bytes_quad32(data[3]);
        buffer128_read(scratch_b, 5, mem) == reverse_bytes_quad32(data[2]);
        buffer128_read(scratch_b, 6, mem) == reverse_bytes_quad32(data[1]);
        buffer128_read(scratch_b, 7, mem) == reverse_bytes_quad32(data[0]);
    ensures
        Xi == reverse_bytes_quad32(ghash_incremental(h_LE, y_prev, data));
        old(buffer128_read(scratch_b, 2, mem)) == buffer128_read(scratch_b, 2, mem);
{
    MulAdd_unroll(hkeys_b, scratch_b, h, prev, data);
    lemma_gf128_low_shift();
    lemma_gf128_degree();
    Reduce(gf128_modulus_low_terms);
    // assert of_quad32(Xi) == mod_rev(128, ghash_unroll_back(h, prev, pdata, 0, 6, 5), gf128_modulus);
    lemma_ghash_unroll_back_forward(h, prev, pdata, 0, 5);
    // assert of_quad32(Xi) == mod_rev(128, ghash_unroll(h, prev, pdata, 0, 5, 0), gf128_modulus);
    lemma_ghash_poly_of_unroll(h, prev, pdata, 0, 5);
    // assert of_quad32(Xi) == ghash_poly(h, prev, pdata, 0, 6);
    lemma_ghash_incremental_poly(h_LE, y_prev, data);
    // assert of_quad32(Xi) == of_quad32(reverse_bytes_quad32(ghash_incremental(h_LE, y_prev, data)));
    lemma_to_of_quad32(reverse_bytes_quad32(ghash_incremental(h_LE, y_prev, data)));
}
