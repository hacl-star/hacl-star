include "../../../../arch/x64/X64.Vale.InsBasic.vaf"
include "../../../../arch/x64/X64.Vale.InsMem.vaf"
include "../../../../arch/x64/X64.Vale.InsVector.vaf"
include "../../../../arch/x64/X64.Vale.InsAes.vaf"

module X64.AESCTR

#verbatim{:interface}{:implementation}
//open Opaque_s
open Words_s
open Types_s
open FStar.Seq
open AES_s
open X64.Machine_s
open X64.Memory
open X64.Vale.State
open X64.Vale.Decls
open X64.Vale.InsBasic
open X64.Vale.InsMem
open X64.Vale.InsVector
open X64.Vale.InsAes
open X64.Vale.QuickCode
open X64.Vale.QuickCodes
open Arch.Types
open AES_helpers
open X64.Poly1305.Math    // For lemma_poly_bits64()
open GCM_helpers
open GCTR_s
open GCTR
open Arch.TypesNative
//open Arch.Types
//open AES256_helpers
#endverbatim

#verbatim{:interface}

let aes_reqs (alg:algorithm) (key:aes_key_LE(alg)) (round_keys:seq(quad32)) (keys_b:buffer128)
             (key_ptr:nat64) (mem:mem) (memTaint:memtaint) : Type0 =
    (alg = AES_128 || alg = AES_256) /\
    length(round_keys) == nr(alg) + 1 /\
    round_keys == key_to_round_keys_LE alg key /\
    key_ptr == buffer_addr keys_b mem /\
    validSrcAddrs128 mem key_ptr keys_b (nr alg + 1) memTaint Secret /\
    buffer128_as_seq mem keys_b == round_keys

#endverbatim

// TODO: Write one procedure that unrolls to these variants?
// These procedures help walk F* through the recursive definition of rounds
procedure {:quick exportOnly} aes_round_4way(
    ghost alg:algorithm,
    ghost n:int,
    round_key:xmm,
    ghost init1:quad32,
    ghost init2:quad32,
    ghost init3:quad32,
    ghost init4:quad32,
    ghost round_keys:seq(quad32)
    )
    modifies
        xmm11; xmm12; xmm13; xmm14; efl;
    requires
        1 <= n < nr(alg) <= length(round_keys);
        xmm11 == rounds(init1, round_keys, n - 1);
        xmm12 == rounds(init2, round_keys, n - 1);
        xmm13 == rounds(init3, round_keys, n - 1);
        xmm14 == rounds(init4, round_keys, n - 1);
        round_key == index(round_keys, n);
        11 != @round_key;
        12 != @round_key;
        13 != @round_key;
        14 != @round_key;
    ensures
        xmm11 == rounds(init1, round_keys, n);
        xmm12 == rounds(init2, round_keys, n);
        xmm13 == rounds(init3, round_keys, n);
        xmm14 == rounds(init4, round_keys, n);
{
    commute_sub_bytes_shift_rows_forall();
    AESNI_enc(xmm11, round_key);
    AESNI_enc(xmm12, round_key);
    AESNI_enc(xmm13, round_key);
    AESNI_enc(xmm14, round_key);
}

procedure {:quick} aes_2rounds_4way(
    ghost alg:algorithm,
    ghost n:int,
    ghost init1:quad32,
    ghost init2:quad32,
    ghost init3:quad32,
    ghost init4:quad32,
    ghost round_keys:seq(quad32)
    )
    reads
        xmm8; xmm9;
    modifies
        xmm11; xmm12; xmm13; xmm14; efl;
    requires
        1 <= n < nr(alg) - 1 && nr(alg) <= length(round_keys);
        xmm11 == rounds(init1, round_keys, n - 1);
        xmm12 == rounds(init2, round_keys, n - 1);
        xmm13 == rounds(init3, round_keys, n - 1);
        xmm14 == rounds(init4, round_keys, n - 1);
        xmm8  == index(round_keys, n);
        xmm9  == index(round_keys, n + 1);
    ensures
        xmm11 == rounds(init1, round_keys, n + 1);
        xmm12 == rounds(init2, round_keys, n + 1);
        xmm13 == rounds(init3, round_keys, n + 1);
        xmm14 == rounds(init4, round_keys, n + 1);
{
    aes_round_4way(alg, n,     xmm8,  init1, init2, init3, init4, round_keys);
    aes_round_4way(alg, n + 1, xmm9,  init1, init2, init3, init4, round_keys);
}

procedure {:quick} aes_3rounds_4way(
    ghost alg:algorithm,
    ghost n:int,
    ghost init1:quad32,
    ghost init2:quad32,
    ghost init3:quad32,
    ghost init4:quad32,
    ghost round_keys:seq(quad32)
    )
    reads
        xmm7; xmm9; xmm10;
    modifies
        xmm11; xmm12; xmm13; xmm14; efl;
    requires
        1 <= n < nr(alg) - 2 && nr(alg) <= length(round_keys);
        xmm11 == rounds(init1, round_keys, n - 1);
        xmm12 == rounds(init2, round_keys, n - 1);
        xmm13 == rounds(init3, round_keys, n - 1);
        xmm14 == rounds(init4, round_keys, n - 1);
        xmm9  == index(round_keys, n + 0);
        xmm10 == index(round_keys, n + 1);
        xmm7  == index(round_keys, n + 2);
    ensures
        xmm11 == rounds(init1, round_keys, n + 2);
        xmm12 == rounds(init2, round_keys, n + 2);
        xmm13 == rounds(init3, round_keys, n + 2);
        xmm14 == rounds(init4, round_keys, n + 2);
{
    aes_round_4way(alg, n + 0, xmm9,  init1, init2, init3, init4, round_keys);
    aes_round_4way(alg, n + 1, xmm10, init1, init2, init3, init4, round_keys);
    aes_round_4way(alg, n + 2, xmm7,  init1, init2, init3, init4, round_keys);
}

procedure {:quick} aes_4rounds_4way(
    ghost alg:algorithm,
    ghost n:int,
    ghost init1:quad32,
    ghost init2:quad32,
    ghost init3:quad32,
    ghost init4:quad32,
    ghost round_keys:seq(quad32)
    )
    reads
        xmm7; xmm8; xmm9; xmm10;
    modifies
        xmm11; xmm12; xmm13; xmm14; efl;
    requires
        1 <= n < nr(alg) - 3 && nr(alg) <= length(round_keys);
        xmm11 == rounds(init1, round_keys, n - 1);
        xmm12 == rounds(init2, round_keys, n - 1);
        xmm13 == rounds(init3, round_keys, n - 1);
        xmm14 == rounds(init4, round_keys, n - 1);
        xmm8  == index(round_keys, n);
        xmm9  == index(round_keys, n + 1);
        xmm10 == index(round_keys, n + 2);
        xmm7  == index(round_keys, n + 3);
    ensures
        xmm11 == rounds(init1, round_keys, n + 3);
        xmm12 == rounds(init2, round_keys, n + 3);
        xmm13 == rounds(init3, round_keys, n + 3);
        xmm14 == rounds(init4, round_keys, n + 3);
{
    aes_round_4way(alg, n,     xmm8,  init1, init2, init3, init4, round_keys);
    aes_round_4way(alg, n + 1, xmm9,  init1, init2, init3, init4, round_keys);
    aes_round_4way(alg, n + 2, xmm10, init1, init2, init3, init4, round_keys);
    aes_round_4way(alg, n + 3, xmm7,  init1, init2, init3, init4, round_keys);
}

#reset-options "--z3rlimit 181"
// Intel's LOOP_4
procedure{:quick} aes_ctr_loop_body(
    inline alg:algorithm,  // Intel simply passes number of rounds (nr) as a dynamic parameter.  Saves code space but adds extra instructions
    ghost old_plain_ptr:nat64,
    ghost old_out_ptr:nat64,
    ghost old_num_quad_blocks:nat64,
    ghost count:nat,
    ghost plain_b:buffer128,
    ghost out_b:buffer128,

    ghost key:aes_key_LE(alg),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128,

    ghost icb_BE:quad32
    )
    lets
        plain_ptr @= rdi; num_quad_blocks @= r8; out_ptr @= rsi; key_ptr @= r9;
        const2_1 @= xmm1; const4_3 @= xmm2; iv @= xmm0; mask64 @= xmm3; four @= xmm4;
        rk1 @= xmm8; rk2 @= xmm9; rk3 @= xmm10; rk4 @= xmm7;        // Round keys
        ctr1 @= xmm11; ctr2 @= xmm12; ctr3 @= xmm13; ctr4 @= xmm14; // Counters being fed to AES
        tmp_xmm @= xmm5;
    requires
        // There's at least one block of four left to do
        0 < num_quad_blocks <= old_num_quad_blocks;
        
        // We've already done count blocks
        count == old_num_quad_blocks - num_quad_blocks;

        // Valid ptrs and buffers
        validSrcAddrs128(mem, old_plain_ptr,  plain_b, old_num_quad_blocks * 4, memTaint, Secret);
        validDstAddrs128(mem, old_out_ptr,    out_b,   old_num_quad_blocks * 4, memTaint, Secret);
        plain_ptr == old_plain_ptr + count * 64;
        out_ptr == old_out_ptr + count * 64;
        old_plain_ptr + old_num_quad_blocks * 64 < pow2_64;
        old_out_ptr + old_num_quad_blocks * 64 < pow2_64;
        buffer_length(plain_b)  <= buffer_length(out_b);
        buffers_disjoint128(out_b, keys_b);

        // XMM constants are correct
        mask64 == Mkfour(0x04050607, 0x00010203, 0x0C0D0E0F, 0x08090A0B);
        four == Mkfour(4, 0, 4, 0);

        // Counters are correct
        4*count < pow2_32 - 4;  // Simplifies the statement of add_wrap in the postcondition below
        const2_1 == Mkfour(iv.hi2, reverse_bytes_nat32(add_wrap(icb_BE.lo0, add_wrap(4*count, 1))), iv.hi2, reverse_bytes_nat32(add_wrap(icb_BE.lo0, add_wrap(4*count, 0))));
        const4_3 == Mkfour(iv.hi2, reverse_bytes_nat32(add_wrap(icb_BE.lo0, add_wrap(4*count, 3))), iv.hi2, reverse_bytes_nat32(add_wrap(icb_BE.lo0, add_wrap(4*count, 2))));

        // AES reqs
        aes_reqs(alg, key, round_keys, keys_b, key_ptr, mem, memTaint);

        // GCM reqs
        iv == reverse_bytes_quad32(icb_BE);

        // GCTR progress
        gctr_partial(alg, 4*count, old(buffer128_as_seq(mem, plain_b)), buffer128_as_seq(mem, out_b), key, icb_BE);

    reads key_ptr; iv; mask64; four; memTaint;
    modifies const2_1; const4_3; rk4; rk1; rk2; rk3; ctr1; ctr2; ctr3; ctr4;
             tmp_xmm; plain_ptr; num_quad_blocks; out_ptr; efl; mem;
    ensures
        // Framing
        modifies_buffer128(out_b, old(mem), mem);

        // Valid ptrs and buffers
        validSrcAddrs128(mem, old_plain_ptr,  plain_b, old_num_quad_blocks * 4, memTaint, Secret);
        validDstAddrs128(mem, old_out_ptr,    out_b,   old_num_quad_blocks * 4, memTaint, Secret);
        plain_ptr == old(plain_ptr) + 64;
        out_ptr   == old(out_ptr)   + 64;
        num_quad_blocks == old(num_quad_blocks) - 1;
        
        // AES reqs
        aes_reqs(alg, key, round_keys, keys_b, key_ptr, mem, memTaint);

        // Counters are incremented
        const2_1 == Mkfour(iv.hi2, reverse_bytes_nat32(add_wrap(icb_BE.lo0, add_wrap(4*count + 4, 1))), iv.hi2, reverse_bytes_nat32(add_wrap(icb_BE.lo0, add_wrap(4*count + 4, 0))));
        const4_3 == Mkfour(iv.hi2, reverse_bytes_nat32(add_wrap(icb_BE.lo0, add_wrap(4*count + 4, 3))), iv.hi2, reverse_bytes_nat32(add_wrap(icb_BE.lo0, add_wrap(4*count + 4, 2))));

        // GCTR progress
        gctr_partial(alg, 4*count+4, old(buffer128_as_seq(mem, plain_b)), buffer128_as_seq(mem, out_b), key, icb_BE);
{
    Mov128(ctr1, iv);        // ctr1==12==13==14 == LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | bswap(icb_BE.lo1) | bswap(icb_BE.lo0)
    Mov128(ctr2, iv);
    Mov128(ctr3, iv);
    Mov128(ctr4, iv);

    Shufpd(ctr1, const2_1, 2); // ctr1 = LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | iv.hi2 == bswap(icb_BE.lo1) | bswap(4*count)
    Shufpd(ctr2, const2_1, 0); // ctr2 = LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | iv.hi2 == bswap(icb_BE.lo1) | bswap(4*count+1)
    Shufpd(ctr3, const4_3, 2); // ctr3 = LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | iv.hi2 == bswap(icb_BE.lo1) | bswap(4*count+2)
    Shufpd(ctr4, const4_3, 0); // ctr4 = LSB: bswap(icb_BE.hi3) | bswap(icb_BE.hi2) | iv.hi2 == bswap(icb_BE.lo1) | bswap(4*count+3)

    reveal_reverse_bytes_quad32(icb_BE);
    reveal_reverse_bytes_quad32(ctr1);
    reveal_reverse_bytes_quad32(ctr2);
    reveal_reverse_bytes_quad32(ctr3);
    reveal_reverse_bytes_quad32(ctr4);

    Pshufb64(const2_1, mask64);     // const2_1 = LSB: 2 | bswap(ivec_hi) | 1 | bswap(ivec_hi)
    Pshufb64(const4_3, mask64);     // const4_3 = LSB: 4 | bswap(ivec_hi) | 3 | bswap(ivec_hi)

    // Load the next four round key blocks
    Load128_buffer(rk1,  key_ptr,  0, Secret, keys_b, 0);
    Load128_buffer(rk2,  key_ptr, 16, Secret, keys_b, 1);
    Load128_buffer(rk3, key_ptr, 32, Secret, keys_b, 2);
    Load128_buffer(rk4,  key_ptr, 48, Secret, keys_b, 3);
    assert{:quick_type} length(round_keys) == nr(alg) + 1;
    assert rk1  == index(round_keys, 0);
    assert rk2  == index(round_keys, 1);
    assert rk3 == index(round_keys, 2);
    assert rk4  == index(round_keys, 3);

    // Pre-emptively increment our counters
    Paddd(const2_1, four);        // const2_1 = LSB: 6 | bswap(ivec_hi) | 5 | bswap(ivec_hi)
    Paddd(const4_3, four);        // const4_3 = LSB: 8 | bswap(ivec_hi) | 7 | bswap(ivec_hi)

    // Begin AES block encrypt by xor'ing four blocks of counters with the key block 0
    // At this point, we're computing aes_encrypt_LE of bswap(inc32(icb_BE, 4*count + 0..3)) == aes_encrypt_BE of inc32(icb_BE, 4*count + 0..3)
    Pxor(ctr1, rk1);
    Pxor(ctr2, rk1);
    Pxor(ctr3, rk1);
    Pxor(ctr4, rk1);
    ghost var init1 := ctr1;
    ghost var init2 := ctr2;
    ghost var init3 := ctr3;
    ghost var init4 := ctr4;

    Pshufb64(const2_1, mask64);      // const2_1 = LSB: ivec_hi | bswap(6) | ivec_hi | bswap(5)
    Pshufb64(const4_3, mask64);      // const4_3 = LSB: ivec_hi | bswap(8) | ivec_hi | bswap(7)

    // Compute three AES rounds for all four counters
    aes_3rounds_4way(alg, 1, init1, init2, init3, init4, round_keys);

    // Load the next four round key blocks
    Load128_buffer(rk1, key_ptr,  64, Secret, keys_b, 4);
    Load128_buffer(rk2, key_ptr,  80, Secret, keys_b, 5);
    Load128_buffer(rk3, key_ptr,  96, Secret, keys_b, 6);
    Load128_buffer(rk4, key_ptr, 112, Secret, keys_b, 7);
    assert rk1 == index(round_keys, 4);
    assert rk2 == index(round_keys, 5);
    assert rk3 == index(round_keys, 6);
    assert rk4 == index(round_keys, 7);

    // Do another 4 AES rounds for each of the four counters = 7 rounds total (not counting xor step)
    aes_4rounds_4way(alg, 4, init1, init2, init3, init4, round_keys);

    // Load the next three round key blocks
    Load128_buffer(rk1, key_ptr, 128, Secret, keys_b, 8);
    Load128_buffer(rk2, key_ptr, 144, Secret, keys_b, 9);
    Load128_buffer(rk3, key_ptr, 160, Secret, keys_b, 10);
    assert rk1  == index(round_keys, 8);
    assert rk2  == index(round_keys, 9);
    assert rk3 == index(round_keys, 10);

    // Do another 2 AES rounds for each of the four counters = 9 rounds total (not counting xor step)
    aes_2rounds_4way(alg, 8, init1, init2, init3, init4, round_keys);

    // TODO: AES_192 and AES_256 would do a few more rounds here
    inline if (alg = AES_256) {
        // Load 3 more round keys and shuffle the one in rk3 to rk1 (Intel does this by reloading from memory -- why?)
        Mov128(rk1, rk3);
        Load128_buffer(rk2, key_ptr, 176, Secret, keys_b, 11);
        Load128_buffer(rk3, key_ptr, 192, Secret, keys_b, 12);
        Load128_buffer(rk4, key_ptr, 208, Secret, keys_b, 13);
        assert{:quick_type} length(round_keys) == 15;
        assert rk1 == index(round_keys, 10);
        assert rk2 == index(round_keys, 11);
        assert rk3 == index(round_keys, 12);
        assert rk4 == index(round_keys, 13);

        // Do another 4 AES rounds for each of the four counters = 13 rounds total (not counting xor step)
        aes_4rounds_4way(alg, 10, init1, init2, init3, init4, round_keys);

        // Load the final round key
        Load128_buffer(rk3, key_ptr, 224, Secret, keys_b, 14);
        assert rk3 == index(round_keys, 14);
    }

    // Intel's LAST_4
    //assert ctr1 == rounds(quad32_xor(old_ctr1, index(round_keys, 0), round_keys, nr(alg) - 1));

    // Compute the last AES round for each of the four counters
    AESNI_enc_last(ctr1, rk3);
    AESNI_enc_last(ctr2, rk3);
    AESNI_enc_last(ctr3, rk3);
    AESNI_enc_last(ctr4, rk3);


    commute_sub_bytes_shift_rows_forall();

    // Xor the plaintext with the encrypted counter
    // TODO: Intel does this using XMM operands
    Load128_buffer(tmp_xmm, plain_ptr,  0, Secret, plain_b, 4*count + 0);
    Pxor(ctr1, tmp_xmm);
    Load128_buffer(tmp_xmm, plain_ptr, 16, Secret, plain_b, 4*count + 1);
    Pxor(ctr2, tmp_xmm);
    Load128_buffer(tmp_xmm, plain_ptr, 32, Secret, plain_b, 4*count + 2);
    Pxor(ctr3, tmp_xmm);
    Load128_buffer(tmp_xmm, plain_ptr, 48, Secret, plain_b, 4*count + 3);
    Pxor(ctr4, tmp_xmm);

    // Store the cipher text in output
    Store128_buffer(out_ptr, ctr1,  0, Secret, out_b, 4*count + 0);
    Store128_buffer(out_ptr, ctr2, 16, Secret, out_b, 4*count + 1);
    Store128_buffer(out_ptr, ctr3, 32, Secret, out_b, 4*count + 2);
    Store128_buffer(out_ptr, ctr4, 48, Secret, out_b, 4*count + 3);

    lemma_quad32_xor_commutes_forall();

    Sub64(r8, 1);
    Add64(plain_ptr, 64);
    Add64(out_ptr, 64);
}

procedure {:quick} aes_counter_loop(
    inline alg:algorithm,
    ghost plain_b:buffer128,
    ghost out_b:buffer128,
    ghost key:aes_key_LE(alg),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    lets plain_ptr @= rdi; num_quad_blocks @= r8; out_ptr @= rsi; key_ptr @= r9;
         const2_1 @= xmm1; const4_3 @= xmm2; iv @= xmm0; mask64 @= xmm3; four @= xmm4;
         rk1 @= xmm8; rk2 @= xmm9; rk3 @= xmm10; rk4 @= xmm7;        // Round keys
         ctr1 @= xmm11; ctr2 @= xmm12; ctr3 @= xmm13; ctr4 @= xmm14; // Counters being fed to AES
         tmp_xmm @= xmm5;
    
    reads key_ptr; iv; memTaint;
    modifies mask64; four; const2_1; const4_3; rk4; rk1; rk2; rk3; ctr1; ctr2; ctr3; ctr4;
             tmp_xmm; 
             rax; plain_ptr; num_quad_blocks; out_ptr; efl; mem;

    requires
        // There's at least one block of four left to do
        0 < num_quad_blocks && 4*num_quad_blocks < pow2_32 - 4;

        // Valid ptrs and buffers
        validSrcAddrs128(mem, plain_ptr,  plain_b, num_quad_blocks * 4, memTaint, Secret);
        validDstAddrs128(mem, out_ptr,    out_b,   num_quad_blocks * 4, memTaint, Secret);
        plain_ptr + num_quad_blocks * 64 < pow2_64;
        out_ptr   + num_quad_blocks * 64 < pow2_64;
        buffer_length(plain_b)  <= buffer_length(out_b);

        buffers_disjoint128(out_b, keys_b);
        buffers_disjoint128(plain_b, out_b);        // TODO: Remove this requirement

        // AES reqs
        aes_reqs(alg, key, round_keys, keys_b, key_ptr, mem, memTaint);

    ensures
        modifies_buffer128(out_b, old(mem), mem);
        validSrcAddrs128(mem, old(out_ptr), out_b, old(num_quad_blocks) * 4, memTaint, Secret);

        plain_ptr == old(plain_ptr) + 64 * old(num_quad_blocks);
        out_ptr   == old(out_ptr)   + 64 * old(num_quad_blocks);

        // GCTR
        gctr_partial(alg, 4*old(num_quad_blocks), old(buffer128_as_seq(mem, plain_b)), buffer128_as_seq(mem, out_b), key, old(reverse_bytes_quad32(iv)));

        // TODO:
        //icb == inc32(old(icb), old(len));

        // TODO:
//        // GHash
//        len == 0 ==> hash == old(hash);
//        len > 0 ==> length(buffer128_as_seq(mem, out_b)) > 0 /\ hash == ghash_incremental(h, old(hash), slice_work_around(buffer128_as_seq(mem, out_b), len));
{
    // Initialize the counters
    Mov128(const2_1, iv);

    InitPshufbDupMask(mask64, rax); // borrow the mask64 xmm for other masks
    PshufbDup(const2_1, mask64);    // const2_1 = LSB: r(iv.hi3) | r(iv.hi2) | r(iv.hi3) | r(iv.h2)
    Mov128(const4_3, const2_1);
    ZeroXmm(tmp_xmm);
    PinsrdImm(tmp_xmm, 1, 2, rax);  // tmp_xmm = LSB: 0 | 0 | 1 | 0
    Paddd(const2_1, tmp_xmm);       // const2_1 = LSB: r(iv.hi3) | r(iv.hi2) | r(iv.hi3) + 1 | r(iv.h2)
    assert const2_1 == Mkfour(reverse_bytes_nat32(iv.hi3),reverse_bytes_nat32(iv.hi2),add_wrap(reverse_bytes_nat32(iv.hi3), 1),reverse_bytes_nat32(iv.hi2));

    PinsrdImm(tmp_xmm, 3, 2, rax);  
    PinsrdImm(tmp_xmm, 2, 0, rax);  // tmp_xmm = LSB: 2 | 0 | 3 | 0
    Paddd(const4_3, tmp_xmm);       // const4_3 = LSB: r(iv.hi3) + 2 | r(iv.hi2) | r(iv.hi3) + 3 | r(iv.h2)

    InitPshufbMask(mask64, rax);    // REVIEW: Require this is already in place?
    reveal_reverse_bytes_quad32(const2_1);
    Pshufb(const2_1, mask64);       // const2_1 = LSB: iv.hi2 | r( r(iv.hi3) + 1) | iv.hi2 | iv.hi3

    reveal_reverse_bytes_quad32(const4_3);
    Pshufb(const4_3, mask64);       // const4_3 = LSB: iv.hi2 | r( r(iv.hi3) + 3) | iv.hi2 | r( r(iv.hi3) + 2)

    reveal_reverse_bytes_quad32(iv);
    reveal_reverse_bytes_quad32(reverse_bytes_quad32(iv));

    // Create the XMM constants
    InitPshufb64Mask(mask64, rax);

    ZeroXmm(four);
    PinsrdImm(four, 4, 0, rax);
    PinsrdImm(four, 4, 2, rax);

    ghost var count:nat := 0;
    ghost var icb_BE:quad32 := reverse_bytes_quad32(iv);
    
    while (num_quad_blocks > 0) 
        invariant 
            0 < old(num_quad_blocks) && 4*old(num_quad_blocks) < pow2_32 - 4;
            0 <= num_quad_blocks <= old(num_quad_blocks);
            count == old(num_quad_blocks) - num_quad_blocks;
            0 <= count <= pow2_32 - 4;
        
            // Framing
            modifies_buffer128(out_b, old(mem), mem);
            buffer128_as_seq(mem, plain_b) == old(buffer128_as_seq(mem, plain_b));  // REVIEW: Shouldn't this follow from above + disjoint?

            // Valid ptrs and buffers
            validSrcAddrs128(mem, old(plain_ptr),  plain_b, old(num_quad_blocks * 4), memTaint, Secret);
            validDstAddrs128(mem, old(out_ptr),    out_b,   old(num_quad_blocks * 4), memTaint, Secret);
            plain_ptr == old(plain_ptr) + count * 64;
            out_ptr == old(out_ptr) + count * 64;
            old(plain_ptr) + old(num_quad_blocks) * 64 < pow2_64;
            old(out_ptr) + old(num_quad_blocks) * 64 < pow2_64;
            buffer_length(plain_b)  <= buffer_length(out_b);
            buffers_disjoint128(out_b, keys_b);
            buffers_disjoint128(plain_b, out_b);        // TODO: Remove this requirement

            // XMM constants are correct
            mask64 == Mkfour(0x04050607, 0x00010203, 0x0C0D0E0F, 0x08090A0B);
            four == Mkfour(4, 0, 4, 0);

            // Counters are correct
            num_quad_blocks > 0 ==> 4*count < pow2_32 - 4;  
            num_quad_blocks <= 0 ==> 4*count < pow2_32;  
            const2_1 == Mkfour(iv.hi2, reverse_bytes_nat32(add_wrap(icb_BE.lo0, add_wrap(4*count, 1))), iv.hi2, reverse_bytes_nat32(add_wrap(icb_BE.lo0, add_wrap(4*count, 0))));
            const4_3 == Mkfour(iv.hi2, reverse_bytes_nat32(add_wrap(icb_BE.lo0, add_wrap(4*count, 3))), iv.hi2, reverse_bytes_nat32(add_wrap(icb_BE.lo0, add_wrap(4*count, 2))));

            // AES reqs
            aes_reqs(alg, key, round_keys, keys_b, key_ptr, mem, memTaint);

            // GCM reqs
            iv == reverse_bytes_quad32(icb_BE);

            // GCTR progress
            gctr_partial(alg, 4*count, old(buffer128_as_seq(mem, plain_b)), buffer128_as_seq(mem, out_b), key, icb_BE);
        decreases num_quad_blocks;
    {
        aes_ctr_loop_body(alg, old(plain_ptr), old(out_ptr), old(num_quad_blocks), count, plain_b, out_b, key, round_keys, keys_b, icb_BE);
        count := count + 1;
    }

}

// TODO: Add XMM memory operands, so we can avoid using xmm3 to hold constants
/*
procedure {:quick} aes_ctr_inner(
    inline alg:algorithm,
    ghost plain_b:buffer128,
    ghost out_b:buffer128,

    ghost key:aes_key_LE(alg),
    ghost round_keys:seq(quad32),
    ghost keys_b:buffer128
    )
    lets
        plain_ptr @= rdi; num_bytes @= r8; out_ptr @= rsi; key_ptr @= r9;
        ivec @= rdx; nonce @= rcx; tmp_xmm @= xmm5; // r12 = num_rounds (nr)
    requires
        validSrcAddrs128(mem, plain_ptr,  plain_b,  bytes_to_quad_size(num_bytes), memTaint, Secret);
        validDstAddrs128(mem, out_ptr, out_b, bytes_to_quad_size(num_bytes), memTaint, Secret);
        plain_ptr  + 16 * bytes_to_quad_size(num_bytes) < pow2_64;
        out_ptr + 16 * bytes_to_quad_size(num_bytes) < pow2_64;
        buffer_length(plain_b)  <= buffer_length(out_b);
        num_bytes < pow2_32;
        nonce < pow2_32;

        // AES reqs
        aes_reqs(alg, key, round_keys, keys_b, key_ptr, mem, memTaint);
    reads key_ptr; ivec; nonce; memTaint;
    modifies rax; r10; xmm0; xmm1; xmm2; xmm3; plain_ptr; num_bytes; out_ptr; tmp_xmm; efl; mem;
    ensures
        true;
{
    lemma_poly_bits64();

    Mov64(r10, r8);
    Shr64(r8, 4);   // r8 := num_bytes / 16 == num_blocks
    And64(r10, 15); // r10 := num_bytes % 16 (note that Intel uses Shl64(r10, 60))
    assert r8 == old(num_bytes) / 16;
    assert r10 == old(num_bytes) % 16;
    if (r10 > 0) {
        Add64(r8, 1);  // Add one to account for trailing blocks
    }
    ghost var num_blocks := r8;

    // Intel: NO_PARTS_4
    Mov64(r10, r8);
    And64(r10, 3); // r10 := num_blocks % 4 (note that Intel uses Shl64(r10, 60); Shr64(r10,60))
    assert r10 == num_blocks % 4;

    // Initialize masks and counters
    Pinsrq(xmm0, nonce, 1);     // Put ivec in the upper 64 bits of xmm0
    Pinsrd(xmm0, rcx, 1);       // Put nonce in the upper 32 bits of the lower 64 bits of xmm0
    Psrldq(xmm0, 4);
    Mov128(xmm2, xmm0);             // Copy xmm0 into xmm2
    InitPshufbDupMask(xmm3, rax);   // xmm3 := dup_mask
    PshufbDup(xmm2, xmm3);          // LSB: 0 | bswap(ivec_hi) | 0 | bswap(ivec_hi)
    ZeroXmm(xmm3);
    PinsrdImm(xmm3, 2, 0, rax);
    PinsrdImm(xmm3, 1, 2, rax);     // xmm3 := TWO_N_ONE
    Paddd(xmm2, xmm3);
    Mov128(xmm1, xmm2);
    PinsrdImm(xmm3, 2, 2, rax);     // xmm3 := TWO_N_TWO
    Paddd(xmm2, xmm3);
    InitPshufb64Mask(xmm3, rax);
    Pshufb64(xmm1, xmm3);           // xmm1 == LSB: ivec_hi | bswap(2) | ivec_hi | bswap(1)
    Pshufb64(xmm2, xmm3);           // xmm2 == LSB: ivec_hi | bswap(4) | ivec_hi | bswap(3)

    Shr64(r8, 2);
    assert r8 == num_blocks / 4;
/*
    if (r8 > 0) {
//        Sub64(plain_ptr, 64);     // TODO: We would need to prove that plain_ptr >= 64...
//        Sub64(out_ptr, 64);
        //aes_ctr_loop();
    }

    // Intel: REMAINDER_4
    if (r10 != 0) {
        Shufpd(xmm0, xmm1, 2);      // xmm0 := LSB: 4 | nonce | ivec + 1

        while (r10 > 0) {
            // BP: This isn't doing anything particularly interesting w.r.t. AES
            //     We can probably replace almost all of this with AESEncryptBlock

            // Intel: IN_LOOP_4
            Mov128(xmm11, xmm0);    // Save a copy of current counter value (xmm0) in xmm11
            Pshufb64(xmm0, mask);

            // Load next four key blocks (TODO: Intel does this via XMM memory operands)
            Load128_buffer(xmm8,  key_ptr,  0, Secret, keys_b, 0);
            Load128_buffer(xmm9,  key_ptr, 16, Secret, keys_b, 1);
            Load128_buffer(xmm10, key_ptr, 32, Secret, keys_b, 2);
            Load128_buffer(xmm7,  key_ptr, 48, Secret, keys_b, 3);

            Pxor(xmm11, xmm8);   // Start encrypting xmm11
            Paddd(xmm0, one);    // Increment the counter by 1
            AESNI_enc(xmm11, xmm9);   // Compute AES round 1
            AESNI_enc(xmm11, xmm10);  // Compute AES round 2
            Pshufb64(xmm0, mask);     // Bswap the upper and lower 64-bit slots of xmm0
            AESNI_enc(xmm11, xmm7);   // Compute AES round 3

            // Load next four key blocks (TODO: Intel does this via XMM memory operands)
            Load128_buffer(xmm8,  key_ptr,  64, Secret, keys_b, 4);
            Load128_buffer(xmm9,  key_ptr,  80, Secret, keys_b, 5);
            Load128_buffer(xmm10, key_ptr,  96, Secret, keys_b, 6);
            Load128_buffer(xmm7,  key_ptr, 112, Secret, keys_b, 7);

            AESNI_enc(xmm11, xmm8);   // Compute AES round 4
            AESNI_enc(xmm11, xmm9);   // Compute AES round 5
            AESNI_enc(xmm11, xmm10);  // Compute AES round 6
            AESNI_enc(xmm11, xmm7);   // Compute AES round 7

            // Load the next three round key blocks
            Load128_buffer(xmm8,  key_ptr, 128, Secret, keys_b, 7);
            Load128_buffer(xmm9,  key_ptr, 144, Secret, keys_b, 8);
            Load128_buffer(xmm10, key_ptr, 160, Secret, keys_b, 9);

            AESNI_enc(xmm11, xmm8);   // Compute AES round 8
            AESNI_enc(xmm11, xmm9);   // Compute AES round 9

            // TODO: AES_192 and AES_256 would do more work here

            // Intel: IN_LAST_4
            AESNI_enc_last(xmm11, xmm10);   // Compute the final AES round
            // Load the plain text
            // TODO: Intel does this using XMM operands
            Load128_buffer(tmp_xmm, plain_ptr,  0, Secret, plain_b, 0); // TODO: Fix indexing
            Pxor(xmm11, tmp_xmm);
            // Store the cipher text in output
            Store128_buffer(out_ptr, xmm11,  0, Secret, out_b, 0);  // TODO: Fix indexing
            Add64(plain_ptr, 16);
            Add64(out_ptr, 16);
            Sub64(r10, 1);
        }
    }
*/
}
*/
